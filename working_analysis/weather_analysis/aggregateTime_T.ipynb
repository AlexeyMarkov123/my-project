{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290bfbc7-3da9-4bd6-8859-6c22d0a94c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac90695-1375-4793-9562-efa1100530c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run weatherDiagramCreator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b6017c-db1e-40a4-83e9-a3f7dedb1572",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if = precip_type_5min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1838\u001b[0m\n\u001b[1;32m   1835\u001b[0m     \u001b[38;5;28mprint\u001b[39m(precipSum)\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1838\u001b[0m     \u001b[43maggregateDiagram_Statistic_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1839\u001b[0m     aggregateDiagram_rain_snow() \u001b[38;5;66;03m# categories\u001b[39;00m\n\u001b[1;32m   1840\u001b[0m     aggregateDiagram_rain_noNaN()\n",
      "Cell \u001b[0;32mIn[1], line 760\u001b[0m, in \u001b[0;36maggregateDiagram_Statistic_all\u001b[0;34m()\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dataFileName\u001b[38;5;241m.\u001b[39mendswith(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m): \u001b[38;5;66;03m# HH_vis_rain_Hamburg_all.parquet\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 760\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_location\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataFileName\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;66;03m# print(dataFileName) #HH_vis_rain_all_P0_Hamburg_all.parquet\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;66;03m# print(\"df: \")\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;66;03m# print(df.head(100)) # 100 Zeilen von oben\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# check = wenn hohe Anzahl an NaNs -> log Darstellung\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# if \"precip_type_5min:idx\" in field and \"precip_5min:mm\" in field:\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecip_type_5min:idx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m field \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecip_5min:mm\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m field:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pandas/io/parquet.py:503\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03mDataFrame\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    501\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pandas/io/parquet.py:251\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    245\u001b[0m     path,\n\u001b[1;32m    246\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    247\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    248\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    249\u001b[0m )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    255\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_as_manager(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pyarrow/parquet/core.py:1811\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[1;32m   1799\u001b[0m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[1;32m   1800\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[1;32m   1801\u001b[0m         source, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[1;32m   1802\u001b[0m         memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1808\u001b[0m         page_checksum_verification\u001b[38;5;241m=\u001b[39mpage_checksum_verification,\n\u001b[1;32m   1809\u001b[0m     )\n\u001b[0;32m-> 1811\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1812\u001b[0m \u001b[43m                    \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pyarrow/parquet/core.py:1454\u001b[0m, in \u001b[0;36mParquetDataset.read\u001b[0;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[1;32m   1446\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1447\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[1;32m   1448\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m   1449\u001b[0m         ]\n\u001b[1;32m   1450\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1451\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[1;32m   1452\u001b[0m         )\n\u001b[0;32m-> 1454\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "# location, areaname, event, time\n",
    "eventStartPattern = re.compile(r'Started event ([A-Za-z0-9_]+) ([A-Za-z0-9_]+) ([A-Za-z0-9_]+) at ([T0-9-:]+)')\n",
    "# location, areaname, event, time\n",
    "eventEndPattern = re.compile(r'Ended event ([A-Za-z0-9_]+) ([A-Za-z0-9_]+) ([A-Za-z0-9_]+) at ([T0-9-:]+)')\n",
    "eventTablePattern = re.compile(r\"([a-zA-Z_0-9]+) +([a-zA-Z_0-9]+) +([a-zA-Z_0-9]+) +([T0-9:-]+) +- +([T0-9:-]+)\")\n",
    "thresholdPattern = re.compile(r\".+_val=([A-Za-z0-9.-]+), .+_threshold=([A-Za-z0-9.-]+)\")\n",
    "\n",
    "def aggregateDiagram_rain_noNaN():\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    a = 0\n",
    "    config = yaml.safe_load(open(\"Diagram_Config_T.yml\", \"r\"))\n",
    "\n",
    "    start = datetime.strptime(config[\"start_date\"], \"%d-%m-%Y %H:%M\")\n",
    "    end = datetime.strptime(config[\"end_date\"], \"%d-%m-%Y %H:%M\")\n",
    "    time_span = end - start # 1096 days, 23:55:00\n",
    "    \n",
    "    for category in config[\"categories\"]:\n",
    "        readableField = None\n",
    "        category = config[\"categories\"][category]\n",
    "        #{'name': 'Rain', 'high_threshold_field': 'precip_type_5min:idx == 1 & precip_5min:mm', 'high_threshold_field_human': 'Rain in mm/5min',\n",
    "        # 'high_threshold_agg': 'max', 'max_display_value_x': 4, 'min_display_value_x': 0, 'max_display_value_y': 20, 'min_display_value_y': 0, 'thresholds': [0.63, 1.67], 'step_size': 0.1}\n",
    "        # print(\"category.keys()\") #dict_keys(['name', 'low_threshold_field', 'low_threshold_agg', 'max_display_value_x', 'min_display_value_x', 'max_display_value_y', 'min_display_value_y', 'thresholds', 'step_size'])\n",
    "\n",
    "        # edit:\n",
    "        # Statistik des Diagrammes auf ges Datensatz beziehen:\n",
    "        if \"low_threshold_agg\" in category.keys() and \"high_threshold_agg\" in category.keys():\n",
    "            aggregate = \"average\"\n",
    "            if category[\"low_threshold_field\"] == category[\"high_threshold_field\"]:\n",
    "                field = category[\"low_threshold_field\"]\n",
    "                if \"low_threshold_field_human\" in category.keys():\n",
    "                    readableField = category[\"low_threshold_field_human\"]\n",
    "                elif \"high_threshold_field_human\" in category.keys():\n",
    "                    readableField = category[\"high_threshold_field_human\"]\n",
    "            else:\n",
    "                raise ValueError(\"Different fields for high and low threshold\")\n",
    "        elif \"low_threshold_agg\" in category.keys():\n",
    "            aggregate = category[\"low_threshold_agg\"]\n",
    "            field = category[\"low_threshold_field\"]\n",
    "            # print(field) #precip_type_5min:idx == 0 & visibility:m\n",
    "            if \"low_threshold_field_human\" in category.keys():\n",
    "                readableField = category[\"low_threshold_field_human\"]\n",
    "            else:\n",
    "                readableField = field\n",
    "        elif \"high_threshold_agg\" in category.keys():\n",
    "            aggregate = category[\"high_threshold_agg\"]\n",
    "            field = category[\"high_threshold_field\"]\n",
    "            # print(aggregate) # max\n",
    "            # print(field) # precip_type_5min:idx == 1 & precip_5min:mm\n",
    "\n",
    "            if \"high_threshold_field_human\" in category.keys():\n",
    "                readableField = category[\"high_threshold_field_human\"]\n",
    "            else:\n",
    "                readableField = field #precip_type_5min:idx == 0 & visibility:m\n",
    "        else:\n",
    "            raise ValueError(\"No aggregate defined\")\n",
    "\n",
    "        field_min = category[\"min_display_value_x\"]\n",
    "        field_max = category[\"max_display_value_x\"]\n",
    "        stepSize = category[\"step_size\"]\n",
    "\n",
    "        bins = []\n",
    "        # orig: -> 1. Bin fehlt\n",
    "        # i = 1\n",
    "        # edit: -> ok für visibility\n",
    "        i = 0 # i = 0 damit x Achse ab 0 startet\n",
    "\n",
    "        while True:\n",
    "            # newBin = field_min-stepSize + i * stepSize\n",
    "            newBin = field_min + i * stepSize # ohne NaNs\n",
    "            if newBin > field_max:\n",
    "                break\n",
    "            bins.append(round(newBin, 4))\n",
    "            i += 1\n",
    "        print(bins) # [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0]\n",
    "        print(stepSize) # 0.1\n",
    "        print(field_min) # 0\n",
    "        print(len(bins)) # 41\n",
    "\n",
    "        numberOfDays = timedelta()\n",
    "        numberOfDays2 = timedelta()\n",
    "\n",
    "        result_df = pd.DataFrame([])\n",
    "        threshold_results = [{\"below\": timedelta(), \"above\": timedelta(), \"equal\": timedelta()} for t in category[\"thresholds\"]]\n",
    "\n",
    "        for dataFileName in os.listdir(config[\"file_location\"]):\n",
    "            if not dataFileName.endswith(config[\"location\"]+ \"_\" + config[\"area\"] + \".parquet\"): # HH_vis_rain_Hamburg_all.parquet\n",
    "                continue\n",
    "            df = pd.read_parquet(os.path.join(config[\"file_location\"],dataFileName))\n",
    "            # print(dataFileName) #HH_vis_rain_all_P0_Hamburg_all.parquet\n",
    "            # print(\"hhhhhh\")\n",
    "            # print(df.head(100)) # 100 Zeilen von oben\n",
    "            # print('df Row count is:',len(df.index)) \n",
    "\n",
    "            # 3        Hamburg       all  2019-03-03T00:00:00+00:00      min                visibility:m  14036.500000\n",
    "            # print(df) #5322235       Hamburg       all  2019-11-02T23:55:00+00:00      max  precip_type_5min:idx == 2 & visibility:m            NaN\n",
    "\n",
    "            # wie soll mit NaNs umgegangen werden? -> löschen oder mit Dummy belegen?\n",
    "\n",
    "            # print(\"field\")\n",
    "            # precip_type_5min:idx == 3 & precip_5min:mm\n",
    "            # precip_type_5min:idx == 0 & visibility:m\n",
    "            # print(field)\n",
    "            if \"precip_type_5min:idx\" in field and \"precip_5min:mm\" in field:\n",
    "                print(\"precip_type_5min\")\n",
    "                df[\"field_value\"] = df[\"field_value\"].fillna(0) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "                # edit für Statistik\n",
    "                field_min_statistic = 0\n",
    "                field_max_statistic = 99999\n",
    "                log = False\n",
    "\n",
    "                # if \"precip_type_5min:idx == 3\" in field:\n",
    "                #     log = True\n",
    "\n",
    "                # start bin size: -0,2 - 0 -> für NaNs\n",
    "                # print(bins) # [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 3.2, 3.4, 3.6, 3.8, 4.0]\n",
    "                # print(stepSize)\n",
    "                # print(bins-stepSize)\n",
    "                # bins = [-0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9]\n",
    "                # [-0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9]\n",
    "                # 0.1\n",
    "                # print('if df Row count is:',len(df[\"field_value\"].index)) \n",
    "\n",
    "                # values = [3, 5, 0, 3, 5, 1, 4, 0, 9]\n",
    "\n",
    "                # def zero_to_nan(values):\n",
    "                #     \"\"\"Replace every 0 with 'nan' and return a copy.\"\"\"\n",
    "                #     return [float('nan') if x==0 else x for x in values]\n",
    "           \n",
    "            elif \"precip_type_5min:idx\" in field and \"visibility:m\" in field:\n",
    "                print(\"visibility:m\")\n",
    "                df[\"field_value\"] = df[\"field_value\"].fillna(99999) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "                # edit für Statistik\n",
    "                field_min_statistic = 0\n",
    "                field_max_statistic = 99999\n",
    "\n",
    "                # print('if df Row count is:',len(df[\"field_value\"].index)) # falsch\n",
    "                           \n",
    "            # df.to_csv(\"df_before_NaN_visib.csv\")\n",
    "            # df[\"field_value\"].to_csv(\"df_fieldvalue_before_NaN.csv\")\n",
    "            \n",
    "            # orig: -> rain, snow\n",
    "            # df[\"field_value\"] = df[\"field_value\"].fillna(0) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "\n",
    "            # df.to_csv(\"df_after_NaN_visib.csv\")\n",
    "\n",
    "            # print(df.head(100))\n",
    "            numberOfDays += timedelta(minutes= 5 * len(df[\"date_time\"].unique()))\n",
    "            #['2018-12-31T00:00:00+00:00' '2018-12-31T00:05:00+00:00'\n",
    "            #'2018-12-31T00:10:00+00:00' ... '2019-11-02T23:45:00+00:00'\n",
    "            # '2019-11-02T23:50:00+00:00' '2019-11-02T23:55:00+00:00']\n",
    "            # print('llll')\n",
    "            # print(5 * len(df[\"date_time\"].unique()))\n",
    "            # print(\"numberOfDays=\")\n",
    "            # print(numberOfDays)\n",
    "            # print(df[\"date_time\"].unique())\n",
    "\n",
    "            # print(field_min_statistic) # 0  # 0\n",
    "            # print(field_max) # 100000 # 2000\n",
    "            # print(\"result_df\")\n",
    "            # print(result_df.head(10))\n",
    "            # print(len(result_df))\n",
    "            #orig:\n",
    "            # result_df = pd.concat([result_df, df[(df[\"field_name\"]==field) & (df[\"agg_name\"] == aggregate) &\n",
    "            #     (df[\"field_value\"] >= field_min) & (df[\"field_value\"] <= field_max)\n",
    "            #     & (df[\"location_name\"] == config[\"location\"]) & (df[\"area_name\"] ==config[\"area\"])]])\n",
    "            \n",
    "            result_df = pd.concat([result_df, df[(df[\"field_name\"]==field) & (df[\"agg_name\"] == aggregate) &\n",
    "                (df[\"field_value\"] >= field_min_statistic) & (df[\"field_value\"] <= field_max_statistic)\n",
    "                & (df[\"location_name\"] == config[\"location\"]) & (df[\"area_name\"] ==config[\"area\"])]])\n",
    "            \n",
    "           \n",
    "            # print(len(result_df))\n",
    "\n",
    "            # print('result_df Row count is:',len(result_df.index))\n",
    "            # result_df.to_csv(\"df_after_NaN_result_rain.csv\")\n",
    "            # print(result_df.head(100))\n",
    "            #      location_name area_name                  date_time agg_name                                  field_name  field_value\n",
    "            #         323         Hamburg       all  2018-12-31T00:00:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            #         659         Hamburg       all  2018-12-31T00:05:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            #         995         Hamburg       all  2018-12-31T00:10:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            \n",
    "            # location_name area_name                  date_time agg_name                                field_name  field_value\n",
    "            # 486227       Hamburg       all  2019-01-05T00:35:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1913.9\n",
    "            # 486563       Hamburg       all  2019-01-05T00:40:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1772.6\n",
    "            # 486899       Hamburg       all  2019-01-05T00:45:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1862.0\n",
    "            # 487235       Hamburg       all  2019-01-05T00:50:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1743.8\n",
    "    \n",
    "            # result = pd.cut(df[\"field_value\"],bins=bins, labels=False )\n",
    "\n",
    "            # for _, row in df.iterrows():\n",
    "            #     pos = math.floor((row[\"field_value\"] - field_min)/stepSize)\n",
    "            #     bins[pos] = bins[pos] + 5\n",
    "        result_df.to_parquet(category[\"name\"] + \"_\" + config[\"location\"] + \".parquet\") #Rain_Hamburg.parquet\n",
    "        result_df.to_csv(category[\"name\"] + \"_\" + config[\"location\"] + \".csv\")\n",
    "\n",
    "        # print(5*len(result_df[result_df[\"field_value\"]]))\n",
    "        # print(result_df[result_df[\"field_value\"]])\n",
    "        # print(len(result_df[\"field_value\"]))\n",
    "        # print(len(result_df[result_df[\"field_value\"]]))\n",
    "\n",
    "        for i,threshold in enumerate(category[\"thresholds\"]):\n",
    "                \n",
    "                threshold_results[i][\"above\"] +=timedelta(minutes=5*len(result_df[result_df[\"field_value\"] > threshold]))\n",
    "                threshold_results[i][\"below\"] += timedelta(minutes=5 * len(result_df[result_df[\"field_value\"] < threshold]))\n",
    "                threshold_results[i][\"equal\"] += timedelta(minutes=5 * len(result_df[result_df[\"field_value\"] == threshold]))\n",
    "\n",
    "        # if a== 0:\n",
    "        #     result_df = pd.read_parquet(\"Snow_\" + config[\"location\"] + \".parquet\")\n",
    "        # else:\n",
    "        #     result_df = pd.read_parquet(\"Rain_\" + config[\"location\"] + \".parquet\")\n",
    "\n",
    "        # plot NaNs = https://matplotlib.org/stable/gallery/lines_bars_and_markers/masked_demo.html\n",
    "        with plt.ioff():\n",
    "            textstr = \"Parameter: {}\\nAggregate: {}\\nLocation: {}\\nArea: {}\\nStart: {}\\nEnd: {}\\n\".format(\n",
    "                readableField, aggregate, config[\"location\"], config[\"area\"], config[\"start_date\"], config[\"end_date\"])\n",
    "            fig, ax = plt.subplots()\n",
    "            print(\"plot\")\n",
    "            # print(field_min) # 0\n",
    "            # orig:\n",
    "            # ax.set_xlim(left=0-stepSize, right=field_max) # 0,4\n",
    "            ax.set_xlim(left=0, right=field_max) # 0,4\n",
    "            # print(0-stepSize) # -0.1\n",
    "            # ax.set_xlim(left=-0.1, right=field_max) # 0,4\n",
    "            # print(field_min) # 0\n",
    "            # print(field_max) # 4\n",
    "            # print(\"max_display_value_y\" in category.keys())\n",
    "\n",
    "            if \"max_display_value_y\" in category.keys() and \"min_display_value_y\" in category.keys():\n",
    "                ax.set_ylim(category[\"min_display_value_y\"], category[\"max_display_value_y\"])\n",
    "                # ax.set_ylim(bottom=category[\"min_display_value_y\"], top=category[\"max_display_value_y\"])\n",
    "            #ax.stairs(bins, [field_min+ i * stepSize for i in range(0, math.ceil((field_max - field_min)/stepSize)+1)], fill=True)\n",
    "\n",
    "            # orig -> bottom=True -> y-Werte starten ab 1\n",
    "            # ax.hist(result_df[\"field_value\"], bins, weights=[1/12]*len(result_df.index), bottom=True, align='mid') # left\n",
    "            # edit\n",
    "            # print([1/12]*len(result_df.index))\n",
    "            # print(result_df.index) #[    321,     657,     993,    1329,    1665,    2001,    2337, 2673,    3009,    3345,\n",
    "            # ...\n",
    "            # 5415969, 5416305, 5416641, 5416977, 5417313, 5417649, 5417985,\n",
    "            # 5418321, 5418657, 5418993]\n",
    "            # print(len(result_df.index)) # 314207\n",
    "\n",
    "            # print(result_df[\"field_value\"])\n",
    "            # https://stackoverflow.com/questions/18697417/not-plotting-zero-in-matplotlib-or-change-zero-to-none-python\n",
    "\n",
    "            \n",
    "            # def zero_to_nan(values):\n",
    "            # # def zero_to_nan(result_df[\"field_value\"]):\n",
    "            #     \"\"\"Replace every 0 with 'nan' and return a copy.\"\"\"\n",
    "            #     return [float('nan') if x==0 else x for x in values]\n",
    "\n",
    "            # print(zero_to_nan(result_df[\"field_value\"]))\n",
    "            # print(\"ffffffff\")\n",
    "            # print(values)\n",
    "            # print(result_df[\"field_value\"])\n",
    "            # print(result_df[\"field_value\"])\n",
    "\n",
    "            result_df[\"field_value\"] = result_df[\"field_value\"].replace(0, -0.01)\n",
    "            # print(data.replace(0, -0.01))\n",
    "\n",
    "            #print(\"after replace\")\n",
    "            # print(result_df[\"field_value\"].head(100))\n",
    "\n",
    "\n",
    "            # values = result_df[\"field_value\"]\n",
    "            # def zero_to_nan(values):\n",
    "            #  # def zero_to_nan(result_df[\"field_value\"]):\n",
    "            #     \"\"\"Replace every 0 with 'nan' and return a copy.\"\"\"\n",
    "            #     return [float('nan') if x==0 else x for x in values]\n",
    "\n",
    "            # # print(zero_to_nan(result_df[\"field_value\"]))\n",
    "\n",
    "            # zero_to_nan(values)\n",
    "            # print(result_df[\"field_value\"])\n",
    "            # print(\"ääääääääääää\")\n",
    "            # print(values)\n",
    "\n",
    "            # # a = [1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1]\n",
    "            # a = result_df[\"field_value\"]\n",
    "            # for i, n in enumerate(a):\n",
    "            #     if n == 0:\n",
    "            #         a[i] = -0.1\n",
    "            #         # print(a)\n",
    "\n",
    "\n",
    "            # ax.hist(result_df[\"field_value\"], bins, weights=[1/12]*len(result_df.index), align='mid') # left\n",
    "            # visualisierung NaNs:\n",
    "            N, bins, patches = ax.hist(result_df[\"field_value\"], bins, weights=[1/12]*len(result_df.index), align='mid')\n",
    "            # print(patches)\n",
    "\n",
    "            # for i in range(0,1):\n",
    "            #     # print(patches[i])\n",
    "            #     patches[i].set_facecolor('g')\n",
    "            # # for i in range(3,5):    \n",
    "            # #     patches[i].set_facecolor('r')\n",
    "            # for i in range(1, len(patches)):\n",
    "            #     patches[i].set_facecolor('blue')\n",
    "\n",
    "            # ax.hist(result_df[\"field_value\"],  align='mid') # left\n",
    "            # ax.hist(data, bins, weights=[1/12]*len(result_df.index), align='mid') # left\n",
    "\n",
    "            #plt.xlim(-0.1, 4)\n",
    "            # plt.show()\n",
    "            # ax.hist(values, bins, weights=[1/12]*len(result_df.index), align='mid') # left\n",
    "            # print(bins) # bei i = 1 -> [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0]\n",
    "            # print(bins) # bei i = 0 -> [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0]\n",
    "            # print(weights=[1/12]*len(result_df.index))\n",
    "\n",
    "            # print(len(result_df.index)) # 314200 länge Datensatz\n",
    "            # print([1/12]*len(result_df.index)) # [0.08333333333333333, 0.08333333333333333, 0.08333333333333333,...]\n",
    "            if \"thresholds\" in category.keys():\n",
    "                for i, threshold in enumerate(category[\"thresholds\"]):\n",
    "                    ax.axvline(threshold, color=\"red\")\n",
    "                    plt.text(threshold, ax.get_ylim()[1] *1.05, str(threshold), ha='center', va='center')\n",
    "                    textstr += \"Threshold: {}\\n  Below: {}\\n  Above: {}\\n  Equal: {}\\n\".format(threshold, threshold_results[i][\"below\"],threshold_results[i][\"above\"], threshold_results[i][\"equal\"] )\n",
    "            plt.text(0.02, 0.3, textstr, fontsize=14, transform=plt.gcf().transFigure)\n",
    "            plt.subplots_adjust(left=0.35, bottom=0.15)\n",
    "            #ax.set_xticks(bins, rotation=90)\n",
    "            # orig:\n",
    "            # bins2 = [bin for i,bin in enumerate(bins) if i % 2 != 0]\n",
    "            # edit:\n",
    "            bins2 = [bin for i,bin in enumerate(bins) if i % 2 != 1] # The % is called the modulo operator. Of course when the remainder is 0, the number is even.\n",
    "            plt.xticks(bins2, bins2, rotation=90)\n",
    "            ax.set_title(\" - \".join([config[\"location\"], config[\"area\"]]), pad=30)\n",
    "            if readableField is not None:\n",
    "                ax.set_xlabel(readableField)\n",
    "            else:\n",
    "                ax.set_xlabel(field)\n",
    "            ax.set_ylabel(\"Hours from {} to {}\".format(config[\"start_date\"][:10], config[\"end_date\"][:10]))\n",
    "            # manager = plt.get_current_fig_manager()\n",
    "            # manager.frame.Maximize(True)\n",
    "            #plt.plot()\n",
    "            if log:\n",
    "                plt.yscale('log')\n",
    "            #plt.show()\n",
    "            plt.savefig(category[\"name\"] + \"_\" + config[\"location\"] + \"_.png\")\n",
    "            quit()\n",
    "            a += 1\n",
    "\n",
    "\n",
    "def aggregateDiagram_rain_snow():\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    a = 0\n",
    "    config = yaml.safe_load(open(\"Diagram_Config_T.yml\", \"r\"))\n",
    "\n",
    "    start = datetime.strptime(config[\"start_date\"], \"%d-%m-%Y %H:%M\")\n",
    "    end = datetime.strptime(config[\"end_date\"], \"%d-%m-%Y %H:%M\")\n",
    "    time_span = end - start # 1096 days, 23:55:00\n",
    "    \n",
    "    for category in config[\"categories\"]:\n",
    "        readableField = None\n",
    "        category = config[\"categories\"][category]\n",
    "        #{'name': 'Rain', 'high_threshold_field': 'precip_type_5min:idx == 1 & precip_5min:mm', 'high_threshold_field_human': 'Rain in mm/5min',\n",
    "        # 'high_threshold_agg': 'max', 'max_display_value_x': 4, 'min_display_value_x': 0, 'max_display_value_y': 20, 'min_display_value_y': 0, 'thresholds': [0.63, 1.67], 'step_size': 0.1}\n",
    "        # print(\"category.keys()\") #dict_keys(['name', 'low_threshold_field', 'low_threshold_agg', 'max_display_value_x', 'min_display_value_x', 'max_display_value_y', 'min_display_value_y', 'thresholds', 'step_size'])\n",
    "\n",
    "        # edit:\n",
    "        # Statistik des Diagrammes auf ges Datensatz beziehen:\n",
    "        if \"low_threshold_agg\" in category.keys() and \"high_threshold_agg\" in category.keys():\n",
    "            aggregate = \"average\"\n",
    "            if category[\"low_threshold_field\"] == category[\"high_threshold_field\"]:\n",
    "                field = category[\"low_threshold_field\"]\n",
    "                if \"low_threshold_field_human\" in category.keys():\n",
    "                    readableField = category[\"low_threshold_field_human\"]\n",
    "                elif \"high_threshold_field_human\" in category.keys():\n",
    "                    readableField = category[\"high_threshold_field_human\"]\n",
    "            else:\n",
    "                raise ValueError(\"Different fields for high and low threshold\")\n",
    "        elif \"low_threshold_agg\" in category.keys():\n",
    "            aggregate = category[\"low_threshold_agg\"]\n",
    "            field = category[\"low_threshold_field\"]\n",
    "            # print(field) #precip_type_5min:idx == 0 & visibility:m\n",
    "            if \"low_threshold_field_human\" in category.keys():\n",
    "                readableField = category[\"low_threshold_field_human\"]\n",
    "            else:\n",
    "                readableField = field\n",
    "        elif \"high_threshold_agg\" in category.keys():\n",
    "            aggregate = category[\"high_threshold_agg\"]\n",
    "            field = category[\"high_threshold_field\"]\n",
    "            # print(aggregate) # max\n",
    "            # print(field) # precip_type_5min:idx == 1 & precip_5min:mm\n",
    "\n",
    "            if \"high_threshold_field_human\" in category.keys():\n",
    "                readableField = category[\"high_threshold_field_human\"]\n",
    "            else:\n",
    "                readableField = field #precip_type_5min:idx == 0 & visibility:m\n",
    "        else:\n",
    "            raise ValueError(\"No aggregate defined\")\n",
    "\n",
    "        field_min = category[\"min_display_value_x\"]\n",
    "        field_max = category[\"max_display_value_x\"]\n",
    "        stepSize = category[\"step_size\"]\n",
    "\n",
    "        bins = []\n",
    "        # orig: -> 1. Bin fehlt\n",
    "        # i = 1\n",
    "        # edit: -> ok für visibility\n",
    "        i = 0 # i = 0 damit x Achse ab 0 startet\n",
    "\n",
    "        while True:\n",
    "            newBin = field_min-stepSize + i * stepSize\n",
    "            \n",
    "            if newBin > field_max:\n",
    "                break\n",
    "            bins.append(round(newBin, 4))\n",
    "            i += 1\n",
    "        print(bins) # [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0]\n",
    "        print(stepSize) # 0.1\n",
    "        print(field_min) # 0\n",
    "        print(len(bins)) # 41\n",
    "\n",
    "        numberOfDays = timedelta()\n",
    "        numberOfDays2 = timedelta()\n",
    "\n",
    "        result_df = pd.DataFrame([])\n",
    "        threshold_results = [{\"below\": timedelta(), \"above\": timedelta(), \"equal\": timedelta()} for t in category[\"thresholds\"]]\n",
    "\n",
    "        for dataFileName in os.listdir(config[\"file_location\"]):\n",
    "            if not dataFileName.endswith(config[\"location\"]+ \"_\" + config[\"area\"] + \".parquet\"): # HH_vis_rain_Hamburg_all.parquet\n",
    "                continue\n",
    "            df = pd.read_parquet(os.path.join(config[\"file_location\"],dataFileName))\n",
    "            # print(dataFileName) #HH_vis_rain_all_P0_Hamburg_all.parquet\n",
    "            # print(df.head(100)) # 100 Zeilen von oben\n",
    "            # print('df Row count is:',len(df.index)) \n",
    "\n",
    "            # 3        Hamburg       all  2019-03-03T00:00:00+00:00      min                visibility:m  14036.500000\n",
    "            # print(df) #5322235       Hamburg       all  2019-11-02T23:55:00+00:00      max  precip_type_5min:idx == 2 & visibility:m            NaN\n",
    "\n",
    "            # wie soll mit NaNs umgegangen werden? -> löschen oder mit Dummy belegen?\n",
    "\n",
    "            # print(\"field\")\n",
    "            # precip_type_5min:idx == 3 & precip_5min:mm\n",
    "            # precip_type_5min:idx == 0 & visibility:m\n",
    "            # print(field)\n",
    "            if \"precip_type_5min:idx\" in field and \"precip_5min:mm\" in field:\n",
    "                print(\"precip_type_5min\")\n",
    "                df[\"field_value\"] = df[\"field_value\"].fillna(0) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "                # edit für Statistik\n",
    "                field_min_statistic = 0\n",
    "                field_max_statistic = 99999\n",
    "                log = False\n",
    "\n",
    "                # if \"precip_type_5min:idx == 3\" in field:\n",
    "                #     log = True\n",
    "\n",
    "                # start bin size: -0,2 - 0 -> für NaNs\n",
    "                # print(bins) # [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 3.2, 3.4, 3.6, 3.8, 4.0]\n",
    "                # print(stepSize)\n",
    "                # print(bins-stepSize)\n",
    "                # bins = [-0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9]\n",
    "                # [-0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9]\n",
    "                # 0.1\n",
    "                # print('if df Row count is:',len(df[\"field_value\"].index)) \n",
    "\n",
    "                # values = [3, 5, 0, 3, 5, 1, 4, 0, 9]\n",
    "\n",
    "                # def zero_to_nan(values):\n",
    "                #     \"\"\"Replace every 0 with 'nan' and return a copy.\"\"\"\n",
    "                #     return [float('nan') if x==0 else x for x in values]\n",
    "           \n",
    "            elif \"precip_type_5min:idx\" in field and \"visibility:m\" in field:\n",
    "                print(\"visibility:m\")\n",
    "                df[\"field_value\"] = df[\"field_value\"].fillna(99999) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "                # edit für Statistik\n",
    "                field_min_statistic = 0\n",
    "                field_max_statistic = 99999\n",
    "\n",
    "                # print('if df Row count is:',len(df[\"field_value\"].index)) # falsch\n",
    "                           \n",
    "            # df.to_csv(\"df_before_NaN_visib.csv\")\n",
    "            # df[\"field_value\"].to_csv(\"df_fieldvalue_before_NaN.csv\")\n",
    "            \n",
    "            # orig: -> rain, snow\n",
    "            # df[\"field_value\"] = df[\"field_value\"].fillna(0) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "\n",
    "            # df.to_csv(\"df_after_NaN_visib.csv\")\n",
    "\n",
    "            # print(df.head(100))\n",
    "            numberOfDays += timedelta(minutes= 5 * len(df[\"date_time\"].unique()))\n",
    "            #['2018-12-31T00:00:00+00:00' '2018-12-31T00:05:00+00:00'\n",
    "            #'2018-12-31T00:10:00+00:00' ... '2019-11-02T23:45:00+00:00'\n",
    "            # '2019-11-02T23:50:00+00:00' '2019-11-02T23:55:00+00:00']\n",
    "            # print('llll')\n",
    "            # print(5 * len(df[\"date_time\"].unique()))\n",
    "            # print(\"numberOfDays=\")\n",
    "            # print(numberOfDays)\n",
    "            # print(df[\"date_time\"].unique())\n",
    "\n",
    "            # print(field_min_statistic) # 0  # 0\n",
    "            # print(field_max) # 100000 # 2000\n",
    "            # print(\"result_df\")\n",
    "            # print(result_df.head(10))\n",
    "            # print(len(result_df))\n",
    "            #orig:\n",
    "            # result_df = pd.concat([result_df, df[(df[\"field_name\"]==field) & (df[\"agg_name\"] == aggregate) &\n",
    "            #     (df[\"field_value\"] >= field_min) & (df[\"field_value\"] <= field_max)\n",
    "            #     & (df[\"location_name\"] == config[\"location\"]) & (df[\"area_name\"] ==config[\"area\"])]])\n",
    "            \n",
    "            result_df = pd.concat([result_df, df[(df[\"field_name\"]==field) & (df[\"agg_name\"] == aggregate) &\n",
    "                (df[\"field_value\"] >= field_min_statistic) & (df[\"field_value\"] <= field_max_statistic)\n",
    "                & (df[\"location_name\"] == config[\"location\"]) & (df[\"area_name\"] ==config[\"area\"])]])\n",
    "            \n",
    "           \n",
    "            # print(len(result_df))\n",
    "\n",
    "            # print('result_df Row count is:',len(result_df.index))\n",
    "            # result_df.to_csv(\"df_after_NaN_result_rain.csv\")\n",
    "            # print(result_df.head(100))\n",
    "            #      location_name area_name                  date_time agg_name                                  field_name  field_value\n",
    "            #         323         Hamburg       all  2018-12-31T00:00:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            #         659         Hamburg       all  2018-12-31T00:05:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            #         995         Hamburg       all  2018-12-31T00:10:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            \n",
    "            # location_name area_name                  date_time agg_name                                field_name  field_value\n",
    "            # 486227       Hamburg       all  2019-01-05T00:35:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1913.9\n",
    "            # 486563       Hamburg       all  2019-01-05T00:40:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1772.6\n",
    "            # 486899       Hamburg       all  2019-01-05T00:45:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1862.0\n",
    "            # 487235       Hamburg       all  2019-01-05T00:50:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1743.8\n",
    "    \n",
    "            # result = pd.cut(df[\"field_value\"],bins=bins, labels=False )\n",
    "\n",
    "            # for _, row in df.iterrows():\n",
    "            #     pos = math.floor((row[\"field_value\"] - field_min)/stepSize)\n",
    "            #     bins[pos] = bins[pos] + 5\n",
    "        result_df.to_parquet(category[\"name\"] + \"_\" + config[\"location\"] + \".parquet\") #Rain_Hamburg.parquet\n",
    "        result_df.to_csv(category[\"name\"] + \"_\" + config[\"location\"] + \"neu.csv\")\n",
    "\n",
    "        # print(5*len(result_df[result_df[\"field_value\"]]))\n",
    "        # print(result_df[result_df[\"field_value\"]])\n",
    "        # print(len(result_df[\"field_value\"]))\n",
    "        # print(len(result_df[result_df[\"field_value\"]]))\n",
    "\n",
    "        for i,threshold in enumerate(category[\"thresholds\"]):\n",
    "                \n",
    "                threshold_results[i][\"above\"] +=timedelta(minutes=5*len(result_df[result_df[\"field_value\"] > threshold]))\n",
    "                threshold_results[i][\"below\"] += timedelta(minutes=5 * len(result_df[result_df[\"field_value\"] < threshold]))\n",
    "                threshold_results[i][\"equal\"] += timedelta(minutes=5 * len(result_df[result_df[\"field_value\"] == threshold]))\n",
    "\n",
    "        # if a== 0:\n",
    "        #     result_df = pd.read_parquet(\"Snow_\" + config[\"location\"] + \".parquet\")\n",
    "        # else:\n",
    "        #     result_df = pd.read_parquet(\"Rain_\" + config[\"location\"] + \".parquet\")\n",
    "\n",
    "        # plot NaNs = https://matplotlib.org/stable/gallery/lines_bars_and_markers/masked_demo.html\n",
    "        with plt.ioff():\n",
    "            textstr = \"Parameter: {}\\nAggregate: {}\\nLocation: {}\\nArea: {}\\nStart: {}\\nEnd: {}\\n\".format(\n",
    "                readableField, aggregate, config[\"location\"], config[\"area\"], config[\"start_date\"], config[\"end_date\"])\n",
    "            fig, ax = plt.subplots()\n",
    "            print(\"plot\")\n",
    "            # print(field_min) # 0\n",
    "            # orig:\n",
    "            ax.set_xlim(left=0-stepSize, right=field_max) # 0,4\n",
    "            # print(0-stepSize) # -0.1\n",
    "            # ax.set_xlim(left=-0.1, right=field_max) # 0,4\n",
    "            # print(field_min) # 0\n",
    "            # print(field_max) # 4\n",
    "            # print(\"max_display_value_y\" in category.keys())\n",
    "\n",
    "            if \"max_display_value_y\" in category.keys() and \"min_display_value_y\" in category.keys():\n",
    "                ax.set_ylim(category[\"min_display_value_y\"], category[\"max_display_value_y\"])\n",
    "                # ax.set_ylim(bottom=category[\"min_display_value_y\"], top=category[\"max_display_value_y\"])\n",
    "            #ax.stairs(bins, [field_min+ i * stepSize for i in range(0, math.ceil((field_max - field_min)/stepSize)+1)], fill=True)\n",
    "\n",
    "            # orig -> bottom=True -> y-Werte starten ab 1\n",
    "            # ax.hist(result_df[\"field_value\"], bins, weights=[1/12]*len(result_df.index), bottom=True, align='mid') # left\n",
    "            # edit\n",
    "            # print([1/12]*len(result_df.index))\n",
    "            # print(result_df.index) #[    321,     657,     993,    1329,    1665,    2001,    2337, 2673,    3009,    3345,\n",
    "            # ...\n",
    "            # 5415969, 5416305, 5416641, 5416977, 5417313, 5417649, 5417985,\n",
    "            # 5418321, 5418657, 5418993]\n",
    "            # print(len(result_df.index)) # 314207\n",
    "\n",
    "            # print(result_df[\"field_value\"])\n",
    "            # https://stackoverflow.com/questions/18697417/not-plotting-zero-in-matplotlib-or-change-zero-to-none-python\n",
    "\n",
    "            \n",
    "            # def zero_to_nan(values):\n",
    "            # # def zero_to_nan(result_df[\"field_value\"]):\n",
    "            #     \"\"\"Replace every 0 with 'nan' and return a copy.\"\"\"\n",
    "            #     return [float('nan') if x==0 else x for x in values]\n",
    "\n",
    "            # print(zero_to_nan(result_df[\"field_value\"]))\n",
    "            # print(\"ffffffff\")\n",
    "            # print(values)\n",
    "            # print(result_df[\"field_value\"])\n",
    "            # print(result_df[\"field_value\"])\n",
    "\n",
    "            result_df[\"field_value\"] = result_df[\"field_value\"].replace(0, -0.01)\n",
    "            # print(data.replace(0, -0.01))\n",
    "\n",
    "            #print(\"after replace\")\n",
    "            # print(result_df[\"field_value\"].head(100))\n",
    "\n",
    "\n",
    "            # values = result_df[\"field_value\"]\n",
    "            # def zero_to_nan(values):\n",
    "            #  # def zero_to_nan(result_df[\"field_value\"]):\n",
    "            #     \"\"\"Replace every 0 with 'nan' and return a copy.\"\"\"\n",
    "            #     return [float('nan') if x==0 else x for x in values]\n",
    "\n",
    "            # # print(zero_to_nan(result_df[\"field_value\"]))\n",
    "\n",
    "            # zero_to_nan(values)\n",
    "            # print(result_df[\"field_value\"])\n",
    "            # print(\"ääääääääääää\")\n",
    "            # print(values)\n",
    "\n",
    "            # # a = [1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1]\n",
    "            # a = result_df[\"field_value\"]\n",
    "            # for i, n in enumerate(a):\n",
    "            #     if n == 0:\n",
    "            #         a[i] = -0.1\n",
    "            #         # print(a)\n",
    "\n",
    "\n",
    "            # ax.hist(result_df[\"field_value\"], bins, weights=[1/12]*len(result_df.index), align='mid') # left\n",
    "\n",
    "            N, bins, patches = ax.hist(result_df[\"field_value\"], bins, weights=[1/12]*len(result_df.index), align='mid')\n",
    "            # print(patches)\n",
    "\n",
    "            for i in range(0,1):\n",
    "                # print(patches[i])\n",
    "                patches[i].set_facecolor('g')\n",
    "            # for i in range(3,5):    \n",
    "            #     patches[i].set_facecolor('r')\n",
    "            for i in range(1, len(patches)):\n",
    "                patches[i].set_facecolor('blue')\n",
    "\n",
    "            # ax.hist(result_df[\"field_value\"],  align='mid') # left\n",
    "            # ax.hist(data, bins, weights=[1/12]*len(result_df.index), align='mid') # left\n",
    "\n",
    "            #plt.xlim(-0.1, 4)\n",
    "            # plt.show()\n",
    "            # ax.hist(values, bins, weights=[1/12]*len(result_df.index), align='mid') # left\n",
    "            # print(bins) # bei i = 1 -> [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0]\n",
    "            # print(bins) # bei i = 0 -> [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0]\n",
    "            # print(weights=[1/12]*len(result_df.index))\n",
    "\n",
    "            # print(len(result_df.index)) # 314200 länge Datensatz\n",
    "            # print([1/12]*len(result_df.index)) # [0.08333333333333333, 0.08333333333333333, 0.08333333333333333,...]\n",
    "            if \"thresholds\" in category.keys():\n",
    "                for i, threshold in enumerate(category[\"thresholds\"]):\n",
    "                    ax.axvline(threshold, color=\"red\")\n",
    "                    plt.text(threshold, ax.get_ylim()[1] *1.05, str(threshold), ha='center', va='center')\n",
    "                    textstr += \"Threshold: {}\\n  Below: {}\\n  Above: {}\\n  Equal: {}\\n\".format(threshold, threshold_results[i][\"below\"],threshold_results[i][\"above\"], threshold_results[i][\"equal\"] )\n",
    "            plt.text(0.02, 0.3, textstr, fontsize=14, transform=plt.gcf().transFigure)\n",
    "            plt.subplots_adjust(left=0.35, bottom=0.15)\n",
    "            #ax.set_xticks(bins, rotation=90)\n",
    "            # orig:\n",
    "            # bins2 = [bin for i,bin in enumerate(bins) if i % 2 != 0]\n",
    "            # edit:\n",
    "            bins2 = [bin for i,bin in enumerate(bins) if i % 2 != 1] # The % is called the modulo operator. Of course when the remainder is 0, the number is even.\n",
    "            plt.xticks(bins2, bins2, rotation=90)\n",
    "            ax.set_title(\" - \".join([config[\"location\"], config[\"area\"]]), pad=30)\n",
    "            if readableField is not None:\n",
    "                ax.set_xlabel(readableField)\n",
    "            else:\n",
    "                ax.set_xlabel(field)\n",
    "            ax.set_ylabel(\"Hours from {} to {}\".format(config[\"start_date\"][:10], config[\"end_date\"][:10]))\n",
    "            # manager = plt.get_current_fig_manager()\n",
    "            # manager.frame.Maximize(True)\n",
    "            #plt.plot()\n",
    "            if log:\n",
    "                plt.yscale('log')\n",
    "            #plt.show()\n",
    "            plt.savefig(category[\"name\"] + \"_\" + config[\"location\"] + \"_.png\")\n",
    "            quit()\n",
    "            a += 1\n",
    "\n",
    "\n",
    "def aggregateDiagram_Statistic_all():\n",
    "    # ignore thresholds\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    a = 0\n",
    "    config = yaml.safe_load(open(\"Diagram_Config_T.yml\", \"r\"))\n",
    "    for category in config[\"categories\"]:\n",
    "        readableField = None\n",
    "        category = config[\"categories\"][category]\n",
    "        # print(\"category\")\n",
    "        # print(category)\n",
    "        #{'name': 'Rain', 'high_threshold_field': 'precip_type_5min:idx == 1 & precip_5min:mm', 'high_threshold_field_human': 'Rain in mm/5min',\n",
    "        # 'high_threshold_agg': 'max', 'max_display_value_x': 4, 'min_display_value_x': 0, 'max_display_value_y': 20, 'min_display_value_y': 0, 'thresholds': [0.63, 1.67], 'step_size': 0.1}\n",
    "\n",
    "        # print(\"category.keys()\") #dict_keys(['name', 'low_threshold_field', 'low_threshold_agg', 'max_display_value_x', 'min_display_value_x', 'max_display_value_y', 'min_display_value_y', 'thresholds', 'step_size'])\n",
    "        # print(category.keys())\n",
    "        if \"low_threshold_agg\" in category.keys() and \"high_threshold_agg\" in category.keys():\n",
    "            aggregate = \"average\"\n",
    "            if category[\"low_threshold_field\"] == category[\"high_threshold_field\"]:\n",
    "                field = category[\"low_threshold_field\"]\n",
    "                if \"low_threshold_field_human\" in category.keys():\n",
    "                    readableField = category[\"low_threshold_field_human\"]\n",
    "                elif \"high_threshold_field_human\" in category.keys():\n",
    "                    readableField = category[\"high_threshold_field_human\"]\n",
    "            else:\n",
    "                raise ValueError(\"Different fields for high and low threshold\")\n",
    "        elif \"low_threshold_agg\" in category.keys():\n",
    "            aggregate = category[\"low_threshold_agg\"]\n",
    "            field = category[\"low_threshold_field\"]\n",
    "            # print(field) #precip_type_5min:idx == 0 & visibility:m\n",
    "            if \"low_threshold_field_human\" in category.keys():\n",
    "                readableField = category[\"low_threshold_field_human\"]\n",
    "            else:\n",
    "                readableField = field\n",
    "        elif \"high_threshold_agg\" in category.keys():\n",
    "            aggregate = category[\"high_threshold_agg\"]\n",
    "            field = category[\"high_threshold_field\"]\n",
    "            # print(aggregate) # max\n",
    "            # print(field) # precip_type_5min:idx == 1 & precip_5min:mm\n",
    "\n",
    "            if \"high_threshold_field_human\" in category.keys():\n",
    "                readableField = category[\"high_threshold_field_human\"]\n",
    "            else:\n",
    "                readableField = field #precip_type_5min:idx == 0 & visibility:m\n",
    "        else:\n",
    "            raise ValueError(\"No aggregate defined\")\n",
    "    \n",
    "\n",
    "        field_min = category[\"min_display_value_x\"]\n",
    "        field_max = category[\"max_display_value_x\"]\n",
    "        stepSize = category[\"step_size\"]\n",
    "\n",
    "        bins = []\n",
    "        # orig: -> 1. Bin fehlt\n",
    "        # i = 1\n",
    "        # edit:\n",
    "        i = 0 # i = 0 damit x Achse ab 0 startet\n",
    "\n",
    "        while True:\n",
    "            newBin = field_min + i * stepSize\n",
    "            \n",
    "            if newBin > field_max:\n",
    "                break\n",
    "            bins.append(round(newBin, 4))\n",
    "            i += 1\n",
    "\n",
    "        numberOfDays = timedelta()\n",
    "        numberOfDays2 = timedelta()\n",
    "\n",
    "        result_df = pd.DataFrame([])\n",
    "        # threshold_results = [{\"below\": timedelta(), \"above\": timedelta(), \"equal\": timedelta()} for t in category[\"thresholds\"]]\n",
    "\n",
    "        for dataFileName in os.listdir(config[\"file_location\"]):\n",
    "            if not dataFileName.endswith(config[\"location\"]+ \"_\" + config[\"area\"] + \".parquet\"): # HH_vis_rain_Hamburg_all.parquet\n",
    "                continue\n",
    "            df = pd.read_parquet(os.path.join(config[\"file_location\"],dataFileName))\n",
    "            # print(dataFileName) #HH_vis_rain_all_P0_Hamburg_all.parquet\n",
    "            # print(\"df: \")\n",
    "            # print(df.head(100)) # 100 Zeilen von oben\n",
    "\n",
    "            # location_name area_name                  date_time agg_name               field_name    field_value\n",
    "            # 0        Hamburg       all  2018-12-31T00:00:00+00:00      min                      lat      53.474071\n",
    "            # 1        Hamburg       all  2018-12-31T00:00:00+00:00      min                      lon       9.872043\n",
    "            # 2        Hamburg       all  2018-12-31T00:00:00+00:00      min           precip_5min:mm       0.000000\n",
    "            # 3        Hamburg       all  2018-12-31T00:00:00+00:00      min  effective_cloud_cover:p      79.400000\n",
    "            # 4        Hamburg       all  2018-12-31T00:00:00+00:00      min          sfc_pressure:Pa  102125.000000\n",
    "\n",
    "            # print(df) #5322235       Hamburg       all  2019-11-02T23:55:00+00:00      max  precip_type_5min:idx == 2 & visibility:m            NaN\n",
    "\n",
    "            # wie soll mit NaNs umgegangen werden? -> löschen oder mit Dummy belegen?\n",
    "\n",
    "            # print(\"field\")\n",
    "            # precip_type_5min:idx == 3 & precip_5min:mm\n",
    "            # precip_type_5min:idx == 0 & visibility:m\n",
    "            # print(field)\n",
    "            # Dummy Werte setzen:\n",
    "\n",
    "            # print(field) # precip_type_5min:idx == 1 & precip_5min:mm\n",
    "            # df.to_csv(\"df_before_NaN_rain.csv\")\n",
    "\n",
    "            # check = wenn hohe Anzahl an NaNs -> log Darstellung\n",
    "            # if \"precip_type_5min:idx\" in field and \"precip_5min:mm\" in field:\n",
    "            if \"precip_type_5min:idx\" in field and \"precip_5min:mm\" in field:\n",
    "                print(\"if = precip_type_5min\")\n",
    "                df[\"field_value\"] = df[\"field_value\"].fillna(0.0) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "                field_value_NaN = 0\n",
    "                log = True\n",
    "            elif \"precip_type_5min:idx\" in field and \"visibility:m\" in field:\n",
    "                print(\"visibility:m\")\n",
    "                df[\"field_value\"] = df[\"field_value\"].fillna(99999) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "                field_value_NaN = 99999\n",
    "                log = False\n",
    "            \n",
    "            # df.to_csv(\"df_after_NaN_rain.csv\")\n",
    "            # df[\"field_value\"].to_csv(\"df_fieldvalue_before_NaN.csv\")\n",
    "            \n",
    "            # orig: -> rain, snow\n",
    "            # df[\"field_value\"] = df[\"field_value\"].fillna(0) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "\n",
    "            # df.to_csv(\"df_after_NaN_visib.csv\")\n",
    "\n",
    "            # print(df.head(100))\n",
    "            numberOfDays += timedelta(minutes= 5 * len(df[\"date_time\"].unique()))\n",
    "            # print(df[\"date_time\"].unique())\n",
    "            #['2018-12-31T00:00:00+00:00' '2018-12-31T00:05:00+00:00'\n",
    "            #'2018-12-31T00:10:00+00:00' ... '2019-11-02T23:45:00+00:00'\n",
    "            # '2019-11-02T23:50:00+00:00' '2019-11-02T23:55:00+00:00']\n",
    "            # print(\"field\")\n",
    "            # print(field) # precip_type_5min:idx == 1 & precip_5min:mm\n",
    "            # print(result_df.head(10))\n",
    "\n",
    "\n",
    "            # result_df = pd.concat([result_df, df[(df[\"field_name\"]==field) & (df[\"agg_name\"] == aggregate) &\n",
    "            #     (df[\"field_value\"] >= field_min) & (df[\"field_value\"] <= field_max)\n",
    "            #     & (df[\"location_name\"] == config[\"location\"]) & (df[\"area_name\"] ==config[\"area\"])]])\n",
    "\n",
    "            result_df = pd.concat([result_df, df[(df[\"field_name\"]==field) & (df[\"agg_name\"] == aggregate) &\n",
    "                (df[\"field_value\"] >= field_min) & \n",
    "                (df[\"location_name\"] == config[\"location\"]) & (df[\"area_name\"] ==config[\"area\"])]])\n",
    "\n",
    "            \n",
    "            # print(\"result_df\")\n",
    "            # result_df.to_csv(\"df_after_NaN_result_rain.csv\")\n",
    "            # print(type(result_df)) # <class 'pandas.core.frame.DataFrame'>\n",
    "            # test = result_df.sort_values(by=['field_value'])\n",
    "            # print(test.head(100))\n",
    "            # print(result_df.head(10))\n",
    "            # print(result_df[\"field_value\"].min())\n",
    "            #      location_name area_name                  date_time agg_name                                  field_name  field_value\n",
    "            #         323         Hamburg       all  2018-12-31T00:00:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            #         659         Hamburg       all  2018-12-31T00:05:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            #         995         Hamburg       all  2018-12-31T00:10:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            \n",
    "            # location_name area_name                  date_time agg_name                                field_name  field_value\n",
    "            # 486227       Hamburg       all  2019-01-05T00:35:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1913.9\n",
    "            # 486563       Hamburg       all  2019-01-05T00:40:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1772.6\n",
    "            # 486899       Hamburg       all  2019-01-05T00:45:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1862.0\n",
    "            # 487235       Hamburg       all  2019-01-05T00:50:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1743.8\n",
    "    \n",
    "            # result = pd.cut(df[\"field_value\"],bins=bins, labels=False )\n",
    "\n",
    "            # for _, row in df.iterrows():\n",
    "            #     pos = math.floor((row[\"field_value\"] - field_min)/stepSize)\n",
    "            #     bins[pos] = bins[pos] + 5\n",
    "        result_df.to_parquet(category[\"name\"] + \"_\" + config[\"location\"] + \"_statistic_all.parquet\") #Rain_Hamburg.parquet\n",
    "        result_df.to_csv(category[\"name\"] + \"_\" + config[\"location\"] + \"_statistic_all.csv\")\n",
    "        print(result_df[\"field_value\"].max())\n",
    "        print(result_df[\"field_value\"].min())\n",
    "        field_max = result_df[\"field_value\"].max()\n",
    "        field_min = result_df[\"field_value\"].min()\n",
    "\n",
    "        # for i,threshold in enumerate(category[\"thresholds\"]):\n",
    "        #         threshold_results[i][\"above\"] +=timedelta(minutes=5*len(result_df[result_df[\"field_value\"] > threshold]))\n",
    "        #         threshold_results[i][\"below\"] += timedelta(minutes=5 * len(result_df[result_df[\"field_value\"] < threshold]))\n",
    "        #         threshold_results[i][\"equal\"] += timedelta(minutes=5 * len(result_df[result_df[\"field_value\"] == threshold]))\n",
    "\n",
    "        # if a== 0:\n",
    "        #     result_df = pd.read_parquet(\"Snow_\" + config[\"location\"] + \".parquet\")\n",
    "        # else:\n",
    "        #     result_df = pd.read_parquet(\"Rain_\" + config[\"location\"] + \".parquet\")\n",
    "\n",
    "        with plt.ioff():\n",
    "            textstr = \"Parameter: {}\\nAggregate: {}\\nLocation: {}\\nArea: {}\\nStart: {}\\nEnd: {}\\nNaN: {}\\nMin: {}\\nMax: {}\\nLog: {}\\n\".format(\n",
    "                readableField, aggregate, config[\"location\"], config[\"area\"], config[\"start_date\"], config[\"end_date\"], field_value_NaN, field_min, field_max, log)\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.set_xlim(left=field_min, right=field_max) # 0,4\n",
    "\n",
    "            # if \"max_display_value_y\" in category.keys() and \"min_display_value_y\" in category.keys():\n",
    "            #     ax.set_ylim(bottom=category[\"min_display_value_y\"], top=category[\"max_display_value_y\"])\n",
    "\n",
    "            #ax.stairs(bins, [field_min+ i * stepSize for i in range(0, math.ceil((field_max - field_min)/stepSize)+1)], fill=True)\n",
    "\n",
    "            # orig -> bottom=True -> y-Werte starten ab 1\n",
    "            # ax.hist(result_df[\"field_value\"], bins, weights=[1/12]*len(result_df.index), bottom=True, align='mid') # left\n",
    "            # edit\n",
    "            # ax.hist(result_df[\"field_value\"], bins, weights=[1/12]*len(result_df.index), align='mid') # left\n",
    "            bins = 40\n",
    "            ax.hist(result_df[\"field_value\"], bins, weights=[1/12]*len(result_df.index), align='mid') # left\n",
    "            # print(bins) # bei i = 1 -> [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0]\n",
    "            # print(bins) # bei i = 0 -> [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0]\n",
    "            # print(weights=[1/12]*len(result_df.index))\n",
    "            # print(len(result_df.index)) # 314200 länge Datensatz\n",
    "            # print([1/12]*len(result_df.index)) # [0.08333333333333333, 0.08333333333333333, 0.08333333333333333,...]\n",
    "            # if \"thresholds\" in category.keys():\n",
    "            #     for i, threshold in enumerate(category[\"thresholds\"]):\n",
    "            #         ax.axvline(threshold, color=\"red\")\n",
    "            #         plt.text(threshold, ax.get_ylim()[1] *1.05, str(threshold), ha='center', va='center')\n",
    "            #         textstr += \"Threshold: {}\\n  Below: {}\\n  Above: {}\\n  Equal: {}\\n\".format(threshold, threshold_results[i][\"below\"],threshold_results[i][\"above\"], threshold_results[i][\"equal\"] )\n",
    "            \n",
    "            plt.text(0.02, 0.3, textstr, fontsize=14, transform=plt.gcf().transFigure)\n",
    "            plt.subplots_adjust(left=0.35, bottom=0.15)\n",
    "            # ax.set_xticks(bins, rotation=90)\n",
    "            # orig:\n",
    "            # bins2 = [bin for i,bin in enumerate(bins) if i % 2 != 0]\n",
    "            # edit:\n",
    "            # bins2 = [bin for i,bin in enumerate(bins) if i % 2 != 1] # The % is called the modulo operator. Of course when the remainder is 0, the number is even.\n",
    "            # plt.xticks(bins, bins, rotation=90)\n",
    "            ax.set_title(\" - \".join([config[\"location\"], config[\"area\"]]), pad=30)\n",
    "            if readableField is not None:\n",
    "                ax.set_xlabel(readableField)\n",
    "            else:\n",
    "                ax.set_xlabel(field)\n",
    "            ax.set_ylabel(\"Hours from {} to {}\".format(config[\"start_date\"][:10], config[\"end_date\"][:10]))\n",
    "            # manager = plt.get_current_fig_manager()\n",
    "            # manager.frame.Maximize(True)\n",
    "            #plt.plot()\n",
    "\n",
    "            if log:\n",
    "                plt.yscale('log')\n",
    "            # else:\n",
    "            #     plt.yscale()\n",
    "\n",
    "\n",
    "            #plt.show()\n",
    "            plt.savefig(category[\"name\"] + \"_\" + config[\"location\"] + \"_.png\")\n",
    "            quit()\n",
    "            a += 1\n",
    "\n",
    "def aggregateDiagram_visibility():\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    a = 0\n",
    "    config = yaml.safe_load(open(\"Diagram_Config_T.yml\", \"r\"))\n",
    "\n",
    "    start = datetime.strptime(config[\"start_date\"], \"%d-%m-%Y %H:%M\")\n",
    "    end = datetime.strptime(config[\"end_date\"], \"%d-%m-%Y %H:%M\")\n",
    "    time_span = end - start # 1096 days, 23:55:00\n",
    "    \n",
    "    for category in config[\"categories\"]:\n",
    "        readableField = None\n",
    "        category = config[\"categories\"][category]\n",
    "        #{'name': 'Rain', 'high_threshold_field': 'precip_type_5min:idx == 1 & precip_5min:mm', 'high_threshold_field_human': 'Rain in mm/5min',\n",
    "        # 'high_threshold_agg': 'max', 'max_display_value_x': 4, 'min_display_value_x': 0, 'max_display_value_y': 20, 'min_display_value_y': 0, 'thresholds': [0.63, 1.67], 'step_size': 0.1}\n",
    "        # print(\"category.keys()\") #dict_keys(['name', 'low_threshold_field', 'low_threshold_agg', 'max_display_value_x', 'min_display_value_x', 'max_display_value_y', 'min_display_value_y', 'thresholds', 'step_size'])\n",
    "\n",
    "        # edit:\n",
    "        # Statistik des Diagrammes auf ges Datensatz beziehen:\n",
    "        if \"low_threshold_agg\" in category.keys() and \"high_threshold_agg\" in category.keys():\n",
    "            aggregate = \"average\"\n",
    "            if category[\"low_threshold_field\"] == category[\"high_threshold_field\"]:\n",
    "                field = category[\"low_threshold_field\"]\n",
    "                if \"low_threshold_field_human\" in category.keys():\n",
    "                    readableField = category[\"low_threshold_field_human\"]\n",
    "                elif \"high_threshold_field_human\" in category.keys():\n",
    "                    readableField = category[\"high_threshold_field_human\"]\n",
    "            else:\n",
    "                raise ValueError(\"Different fields for high and low threshold\")\n",
    "        elif \"low_threshold_agg\" in category.keys():\n",
    "            aggregate = category[\"low_threshold_agg\"]\n",
    "            field = category[\"low_threshold_field\"]\n",
    "            # print(field) #precip_type_5min:idx == 0 & visibility:m\n",
    "            if \"low_threshold_field_human\" in category.keys():\n",
    "                readableField = category[\"low_threshold_field_human\"]\n",
    "            else:\n",
    "                readableField = field\n",
    "        elif \"high_threshold_agg\" in category.keys():\n",
    "            aggregate = category[\"high_threshold_agg\"]\n",
    "            field = category[\"high_threshold_field\"]\n",
    "            # print(aggregate) # max\n",
    "            # print(field) # precip_type_5min:idx == 1 & precip_5min:mm\n",
    "\n",
    "            if \"high_threshold_field_human\" in category.keys():\n",
    "                readableField = category[\"high_threshold_field_human\"]\n",
    "            else:\n",
    "                readableField = field #precip_type_5min:idx == 0 & visibility:m\n",
    "        else:\n",
    "            raise ValueError(\"No aggregate defined\")\n",
    "\n",
    "        field_min = category[\"min_display_value_x\"]\n",
    "        field_max = category[\"max_display_value_x\"]\n",
    "        stepSize = category[\"step_size\"]\n",
    "\n",
    "        bins = []\n",
    "        # orig: -> 1. Bin fehlt\n",
    "        # i = 1\n",
    "        # edit: -> ok für visibility\n",
    "        i = 0 # i = 0 damit x Achse ab 0 startet\n",
    "\n",
    "        while True:\n",
    "            newBin = field_min + i * stepSize\n",
    "            \n",
    "            if newBin > field_max:\n",
    "                break\n",
    "            bins.append(round(newBin, 4))\n",
    "            i += 1\n",
    "        # print(bins) # [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 3.2, 3.4, 3.6, 3.8, 4.0]\n",
    "        # print(stepSize)\n",
    "\n",
    "        numberOfDays = timedelta()\n",
    "        numberOfDays2 = timedelta()\n",
    "\n",
    "        result_df = pd.DataFrame([])\n",
    "        threshold_results = [{\"below\": timedelta(), \"above\": timedelta(), \"equal\": timedelta()} for t in category[\"thresholds\"]]\n",
    "\n",
    "        for dataFileName in os.listdir(config[\"file_location\"]):\n",
    "            if not dataFileName.endswith(config[\"location\"]+ \"_\" + config[\"area\"] + \".parquet\"): # HH_vis_rain_Hamburg_all.parquet\n",
    "                continue\n",
    "            df = pd.read_parquet(os.path.join(config[\"file_location\"],dataFileName))\n",
    "            # print(dataFileName) #HH_vis_rain_all_P0_Hamburg_all.parquet\n",
    "            # print(df.head(100)) # 100 Zeilen von oben\n",
    "            # print('df Row count is:',len(df.index)) \n",
    "\n",
    "            # 3        Hamburg       all  2019-03-03T00:00:00+00:00      min                visibility:m  14036.500000\n",
    "            # print(df) #5322235       Hamburg       all  2019-11-02T23:55:00+00:00      max  precip_type_5min:idx == 2 & visibility:m            NaN\n",
    "\n",
    "            # wie soll mit NaNs umgegangen werden? -> löschen oder mit Dummy belegen?\n",
    "\n",
    "            # print(\"field\")\n",
    "            # precip_type_5min:idx == 3 & precip_5min:mm\n",
    "            # precip_type_5min:idx == 0 & visibility:m\n",
    "            # print(field)\n",
    "            if \"precip_type_5min:idx\" in field and \"precip_5min:mm\" in field:\n",
    "                print(\"precip_type_5min\")\n",
    "                df[\"field_value\"] = df[\"field_value\"].fillna(0) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "                # edit für Statistik\n",
    "                field_min_statistic = 0\n",
    "                field_max_statistic = 99999\n",
    "                # start bin size: -0,2 - 0 -> für NaNs\n",
    "                # print(bins) # [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 3.2, 3.4, 3.6, 3.8, 4.0]\n",
    "                # print(stepSize)\n",
    "                # print(bins-stepSize)\n",
    "                # bins = [-0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9]\n",
    "                # [-0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9]\n",
    "                # 0.1\n",
    "                # print('if df Row count is:',len(df[\"field_value\"].index)) \n",
    "\n",
    "                # values = [3, 5, 0, 3, 5, 1, 4, 0, 9]\n",
    "\n",
    "                # def zero_to_nan(values):\n",
    "                #     \"\"\"Replace every 0 with 'nan' and return a copy.\"\"\"\n",
    "                #     return [float('nan') if x==0 else x for x in values]\n",
    "\n",
    "            # elif \"precip_type_5min:idx\" in field and \"visibility:m\" in field:\n",
    "            elif \"visibility:m\" in field:\n",
    "                print(\"visibility:m\")\n",
    "                df[\"field_value\"] = df[\"field_value\"].fillna(99999) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "                # edit für Statistik\n",
    "                field_min_statistic = 0\n",
    "                field_max_statistic = 99999\n",
    "\n",
    "                # print('if df Row count is:',len(df[\"field_value\"].index)) # falsch\n",
    "                           \n",
    "            # df.to_csv(\"df_before_NaN_visib.csv\")\n",
    "            # df[\"field_value\"].to_csv(\"df_fieldvalue_before_NaN.csv\")\n",
    "            \n",
    "            # orig: -> rain, snow\n",
    "            # df[\"field_value\"] = df[\"field_value\"].fillna(0) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "\n",
    "            # df.to_csv(\"df_after_NaN_visib.csv\")\n",
    "\n",
    "            # print(df.head(100))\n",
    "            numberOfDays += timedelta(minutes= 5 * len(df[\"date_time\"].unique()))\n",
    "            #['2018-12-31T00:00:00+00:00' '2018-12-31T00:05:00+00:00'\n",
    "            #'2018-12-31T00:10:00+00:00' ... '2019-11-02T23:45:00+00:00'\n",
    "            # '2019-11-02T23:50:00+00:00' '2019-11-02T23:55:00+00:00']\n",
    "            # print('llll')\n",
    "            # print(5 * len(df[\"date_time\"].unique()))\n",
    "            # print(\"numberOfDays=\")\n",
    "            # print(numberOfDays)\n",
    "            # print(df[\"date_time\"].unique())\n",
    "\n",
    "            # print(field_min_statistic) # 0  # 0\n",
    "            # print(field_max) # 100000 # 2000\n",
    "            # print(\"result_df\")\n",
    "            # print(result_df.head(10))\n",
    "            # print(len(result_df))\n",
    "            #orig:\n",
    "            # result_df = pd.concat([result_df, df[(df[\"field_name\"]==field) & (df[\"agg_name\"] == aggregate) &\n",
    "            #     (df[\"field_value\"] >= field_min) & (df[\"field_value\"] <= field_max)\n",
    "            #     & (df[\"location_name\"] == config[\"location\"]) & (df[\"area_name\"] ==config[\"area\"])]])\n",
    "            \n",
    "            result_df = pd.concat([result_df, df[(df[\"field_name\"]==field) & (df[\"agg_name\"] == aggregate) &\n",
    "                (df[\"field_value\"] >= field_min_statistic) & (df[\"field_value\"] <= field_max_statistic)\n",
    "                & (df[\"location_name\"] == config[\"location\"]) & (df[\"area_name\"] ==config[\"area\"])]])\n",
    "            \n",
    "           \n",
    "            # print(len(result_df))\n",
    "\n",
    "            # print('result_df Row count is:',len(result_df.index))\n",
    "            # result_df.to_csv(\"df_after_NaN_result_rain.csv\")\n",
    "            # print(result_df.head(100))\n",
    "            #      location_name area_name                  date_time agg_name                                  field_name  field_value\n",
    "            #         323         Hamburg       all  2018-12-31T00:00:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            #         659         Hamburg       all  2018-12-31T00:05:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            #         995         Hamburg       all  2018-12-31T00:10:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            \n",
    "            # location_name area_name                  date_time agg_name                                field_name  field_value\n",
    "            # 486227       Hamburg       all  2019-01-05T00:35:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1913.9\n",
    "            # 486563       Hamburg       all  2019-01-05T00:40:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1772.6\n",
    "            # 486899       Hamburg       all  2019-01-05T00:45:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1862.0\n",
    "            # 487235       Hamburg       all  2019-01-05T00:50:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1743.8\n",
    "    \n",
    "            # result = pd.cut(df[\"field_value\"],bins=bins, labels=False )\n",
    "\n",
    "            # for _, row in df.iterrows():\n",
    "            #     pos = math.floor((row[\"field_value\"] - field_min)/stepSize)\n",
    "            #     bins[pos] = bins[pos] + 5\n",
    "        result_df.to_parquet(category[\"name\"] + \"_\" + config[\"location\"] + \".parquet\") #Rain_Hamburg.parquet\n",
    "        result_df.to_csv(category[\"name\"] + \"_\" + config[\"location\"] + \".csv\")\n",
    "\n",
    "        # print(5*len(result_df[result_df[\"field_value\"]]))\n",
    "        # print(result_df[result_df[\"field_value\"]])\n",
    "        # print(len(result_df[\"field_value\"]))\n",
    "        # print(len(result_df[result_df[\"field_value\"]]))\n",
    "\n",
    "        for i,threshold in enumerate(category[\"thresholds\"]):\n",
    "                \n",
    "                threshold_results[i][\"above\"] +=timedelta(minutes=5*len(result_df[result_df[\"field_value\"] > threshold]))\n",
    "                threshold_results[i][\"below\"] += timedelta(minutes=5 * len(result_df[result_df[\"field_value\"] < threshold]))\n",
    "                threshold_results[i][\"equal\"] += timedelta(minutes=5 * len(result_df[result_df[\"field_value\"] == threshold]))\n",
    "\n",
    "        # if a== 0:\n",
    "        #     result_df = pd.read_parquet(\"Snow_\" + config[\"location\"] + \".parquet\")\n",
    "        # else:\n",
    "        #     result_df = pd.read_parquet(\"Rain_\" + config[\"location\"] + \".parquet\")\n",
    "\n",
    "        # plot NaNs = https://matplotlib.org/stable/gallery/lines_bars_and_markers/masked_demo.html\n",
    "        with plt.ioff():\n",
    "            textstr = \"Parameter: {}\\nAggregate: {}\\nLocation: {}\\nArea: {}\\nStart: {}\\nEnd: {}\\n\".format(\n",
    "                readableField, aggregate, config[\"location\"], config[\"area\"], config[\"start_date\"], config[\"end_date\"])\n",
    "            fig, ax = plt.subplots()\n",
    "            # orig:\n",
    "            ax.set_xlim(left=field_min, right=field_max) # 0,4\n",
    "            # ax.set_xlim(left=-0.1, right=field_max) # 0,4\n",
    "            # print(field_min) # 0\n",
    "            # print(field_max) # 4\n",
    "            # print(\"max_display_value_y\" in category.keys())\n",
    "\n",
    "            if \"max_display_value_y\" in category.keys() and \"min_display_value_y\" in category.keys():\n",
    "                ax.set_ylim(bottom=category[\"min_display_value_y\"], top=category[\"max_display_value_y\"])\n",
    "            #ax.stairs(bins, [field_min+ i * stepSize for i in range(0, math.ceil((field_max - field_min)/stepSize)+1)], fill=True)\n",
    "\n",
    "            # orig -> bottom=True -> y-Werte starten ab 1\n",
    "            # ax.hist(result_df[\"field_value\"], bins, weights=[1/12]*len(result_df.index), bottom=True, align='mid') # left\n",
    "            # edit\n",
    "            # print([1/12]*len(result_df.index))\n",
    "            # print(result_df.index) #[    321,     657,     993,    1329,    1665,    2001,    2337, 2673,    3009,    3345,\n",
    "            # ...\n",
    "            # 5415969, 5416305, 5416641, 5416977, 5417313, 5417649, 5417985,\n",
    "            # 5418321, 5418657, 5418993]\n",
    "            # print(len(result_df.index)) # 314207\n",
    "\n",
    "            # print(result_df[\"field_value\"])\n",
    "            # https://stackoverflow.com/questions/18697417/not-plotting-zero-in-matplotlib-or-change-zero-to-none-python\n",
    "\n",
    "            # # values = [3, 5, 0, 3, 5, 1, 4, 0, 9]\n",
    "            # values = result_df[\"field_value\"]\n",
    "\n",
    "            # def zero_to_nan(values):\n",
    "            # # def zero_to_nan(result_df[\"field_value\"]):\n",
    "            #     \"\"\"Replace every 0 with 'nan' and return a copy.\"\"\"\n",
    "            #     return [float('nan') if x==0 else x for x in values]\n",
    "\n",
    "            # print(zero_to_nan(result_df[\"field_value\"]))\n",
    "            # print(\"ffffffff\")\n",
    "            # print(values)\n",
    "\n",
    "\n",
    "            ax.hist(result_df[\"field_value\"], bins, weights=[1/12]*len(result_df.index), align='mid') # left\n",
    "            # ax.hist(values, bins, weights=[1/12]*len(result_df.index), align='mid') # left\n",
    "            # print(bins) # bei i = 1 -> [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0]\n",
    "            # print(bins) # bei i = 0 -> [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0]\n",
    "            # print(weights=[1/12]*len(result_df.index))\n",
    "\n",
    "            # print(len(result_df.index)) # 314200 länge Datensatz\n",
    "            # print([1/12]*len(result_df.index)) # [0.08333333333333333, 0.08333333333333333, 0.08333333333333333,...]\n",
    "            if \"thresholds\" in category.keys():\n",
    "                for i, threshold in enumerate(category[\"thresholds\"]):\n",
    "                    ax.axvline(threshold, color=\"red\")\n",
    "                    plt.text(threshold, ax.get_ylim()[1] *1.05, str(threshold), ha='center', va='center')\n",
    "                    textstr += \"Threshold: {}\\n  Below: {}\\n  Above: {}\\n  Equal: {}\\n\".format(threshold, threshold_results[i][\"below\"],threshold_results[i][\"above\"], threshold_results[i][\"equal\"] )\n",
    "            plt.text(0.02, 0.3, textstr, fontsize=14, transform=plt.gcf().transFigure)\n",
    "            plt.subplots_adjust(left=0.35, bottom=0.15)\n",
    "            #ax.set_xticks(bins, rotation=90)\n",
    "            # orig:\n",
    "            # bins2 = [bin for i,bin in enumerate(bins) if i % 2 != 0]\n",
    "            # edit:\n",
    "            bins2 = [bin for i,bin in enumerate(bins) if i % 2 != 1] # The % is called the modulo operator. Of course when the remainder is 0, the number is even.\n",
    "            plt.xticks(bins2, bins2, rotation=90)\n",
    "            ax.set_title(\" - \".join([config[\"location\"], config[\"area\"]]), pad=50)\n",
    "            if readableField is not None:\n",
    "                ax.set_xlabel(readableField)\n",
    "            else:\n",
    "                ax.set_xlabel(field)\n",
    "            ax.set_ylabel(\"Hours from {} to {}\".format(config[\"start_date\"][:10], config[\"end_date\"][:10]))\n",
    "            # manager = plt.get_current_fig_manager()\n",
    "            # manager.frame.Maximize(True)\n",
    "            #plt.plot()\n",
    "            #plt.show()\n",
    "            plt.savefig(category[\"name\"] + \"_\" + config[\"location\"] + \"_.png\")\n",
    "            quit()\n",
    "            a += 1\n",
    "\n",
    "\n",
    "def aggregateDiagram_origJohannes():\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    a = 0\n",
    "    config = yaml.safe_load(open(\"Diagram_Config_T.yml\", \"r\"))\n",
    "    for category in config[\"categories\"]:\n",
    "        readableField = None\n",
    "        category = config[\"categories\"][category]\n",
    "        # print(\"category\")\n",
    "        # print(category)\n",
    "        #{'name': 'Rain', 'high_threshold_field': 'precip_type_5min:idx == 1 & precip_5min:mm', 'high_threshold_field_human': 'Rain in mm/5min',\n",
    "        # 'high_threshold_agg': 'max', 'max_display_value_x': 4, 'min_display_value_x': 0, 'max_display_value_y': 20, 'min_display_value_y': 0, 'thresholds': [0.63, 1.67], 'step_size': 0.1}\n",
    "\n",
    "        # print(\"category.keys()\") #dict_keys(['name', 'low_threshold_field', 'low_threshold_agg', 'max_display_value_x', 'min_display_value_x', 'max_display_value_y', 'min_display_value_y', 'thresholds', 'step_size'])\n",
    "        # print(category.keys())\n",
    "        if \"low_threshold_agg\" in category.keys() and \"high_threshold_agg\" in category.keys():\n",
    "            aggregate = \"average\"\n",
    "            if category[\"low_threshold_field\"] == category[\"high_threshold_field\"]:\n",
    "                field = category[\"low_threshold_field\"]\n",
    "                if \"low_threshold_field_human\" in category.keys():\n",
    "                    readableField = category[\"low_threshold_field_human\"]\n",
    "                elif \"high_threshold_field_human\" in category.keys():\n",
    "                    readableField = category[\"high_threshold_field_human\"]\n",
    "            else:\n",
    "                raise ValueError(\"Different fields for high and low threshold\")\n",
    "        elif \"low_threshold_agg\" in category.keys():\n",
    "            # print(\"low_threshold_agg\")\n",
    "            aggregate = category[\"low_threshold_agg\"]\n",
    "            field = category[\"low_threshold_field\"]\n",
    "            # print(\"field\")\n",
    "            # print(field) #precip_type_5min:idx == 0 & visibility:m\n",
    "            if \"low_threshold_field_human\" in category.keys():\n",
    "                readableField = category[\"low_threshold_field_human\"]\n",
    "            else:\n",
    "                readableField = field\n",
    "        elif \"high_threshold_agg\" in category.keys():\n",
    "            aggregate = category[\"high_threshold_agg\"]\n",
    "            field = category[\"high_threshold_field\"]\n",
    "            # print(aggregate) # max\n",
    "            # print(field) # precip_type_5min:idx == 1 & precip_5min:mm\n",
    "\n",
    "            if \"high_threshold_field_human\" in category.keys():\n",
    "                readableField = category[\"high_threshold_field_human\"]\n",
    "            else:\n",
    "                readableField = field #precip_type_5min:idx == 0 & visibility:m\n",
    "        else:\n",
    "            raise ValueError(\"No aggregate defined\")\n",
    "\n",
    "        field_min = category[\"min_display_value_x\"]\n",
    "        field_max = category[\"max_display_value_x\"]\n",
    "        stepSize = category[\"step_size\"]\n",
    "\n",
    "        bins = []\n",
    "        # orig: -> 1. Bin fehlt\n",
    "        # i = 1\n",
    "        # edit:\n",
    "        i = 0 # i = 0 damit x Achse ab 0 startet\n",
    "\n",
    "        while True:\n",
    "            newBin = field_min + i * stepSize\n",
    "            \n",
    "            if newBin > field_max:\n",
    "                break\n",
    "            bins.append(round(newBin, 4))\n",
    "            i += 1\n",
    "\n",
    "        numberOfDays = timedelta()\n",
    "        numberOfDays2 = timedelta()\n",
    "\n",
    "        result_df = pd.DataFrame([])\n",
    "        threshold_results = [{\"below\": timedelta(), \"above\": timedelta(), \"equal\": timedelta()} for t in category[\"thresholds\"]]\n",
    "\n",
    "        for dataFileName in os.listdir(config[\"file_location\"]):\n",
    "            if not dataFileName.endswith(config[\"location\"]+ \"_\" + config[\"area\"] + \".parquet\"): # HH_vis_rain_Hamburg_all.parquet\n",
    "                continue\n",
    "            df = pd.read_parquet(os.path.join(config[\"file_location\"],dataFileName))\n",
    "            # print(dataFileName) #HH_vis_rain_all_P0_Hamburg_all.parquet\n",
    "            print(df.head(100)) # 100 Zeilen von oben\n",
    "            # 3        Hamburg       all  2019-03-03T00:00:00+00:00      min                visibility:m  14036.500000\n",
    "            # print(df) #5322235       Hamburg       all  2019-11-02T23:55:00+00:00      max  precip_type_5min:idx == 2 & visibility:m            NaN\n",
    "\n",
    "            # wie soll mit NaNs umgegangen werden? -> löschen oder mit Dummy belegen?\n",
    "\n",
    "            # print(\"field\")\n",
    "            # precip_type_5min:idx == 3 & precip_5min:mm\n",
    "            # precip_type_5min:idx == 0 & visibility:m\n",
    "            print(field)\n",
    "            if \"precip_type_5min:idx\" in field and \"precip_5min:mm\" in field:\n",
    "                print(\"precip_type_5min\")\n",
    "                df[\"field_value\"] = df[\"field_value\"].fillna(0) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "            elif \"precip_type_5min:idx\" in field and \"visibility:m\" in field:\n",
    "                print(\"visibility:m\")\n",
    "                df[\"field_value\"] = df[\"field_value\"].fillna(99999) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "\n",
    "            # df.to_csv(\"df_before_NaN_visib.csv\")\n",
    "            # df[\"field_value\"].to_csv(\"df_fieldvalue_before_NaN.csv\")\n",
    "            \n",
    "            # orig: -> rain, snow\n",
    "            # df[\"field_value\"] = df[\"field_value\"].fillna(0) # rain NaNs -> ca. 70%, snow => ca. 98%, visibility = 3%\n",
    "\n",
    "            # df.to_csv(\"df_after_NaN_visib.csv\")\n",
    "\n",
    "            # print(df.head(100))\n",
    "            numberOfDays += timedelta(minutes= 5 * len(df[\"date_time\"].unique()))\n",
    "            # print(df[\"date_time\"].unique())\n",
    "            #['2018-12-31T00:00:00+00:00' '2018-12-31T00:05:00+00:00'\n",
    "            #'2018-12-31T00:10:00+00:00' ... '2019-11-02T23:45:00+00:00'\n",
    "            # '2019-11-02T23:50:00+00:00' '2019-11-02T23:55:00+00:00']\n",
    "\n",
    "            result_df = pd.concat([result_df, df[(df[\"field_name\"]==field) & (df[\"agg_name\"] == aggregate) &\n",
    "                (df[\"field_value\"] >= field_min) & (df[\"field_value\"] <= field_max)\n",
    "                & (df[\"location_name\"] == config[\"location\"]) & (df[\"area_name\"] ==config[\"area\"])]])\n",
    "            \n",
    "            #   print(\"result_df\")\n",
    "            # result_df.to_csv(\"df_after_NaN_result_rain.csv\")\n",
    "            # print(result_df.head(100))\n",
    "            #      location_name area_name                  date_time agg_name                                  field_name  field_value\n",
    "            #         323         Hamburg       all  2018-12-31T00:00:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            #         659         Hamburg       all  2018-12-31T00:05:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            #         995         Hamburg       all  2018-12-31T00:10:00+00:00      max  precip_type_5min:idx == 3 & precip_5min:mm          0.0\n",
    "            \n",
    "            # location_name area_name                  date_time agg_name                                field_name  field_value\n",
    "            # 486227       Hamburg       all  2019-01-05T00:35:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1913.9\n",
    "            # 486563       Hamburg       all  2019-01-05T00:40:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1772.6\n",
    "            # 486899       Hamburg       all  2019-01-05T00:45:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1862.0\n",
    "            # 487235       Hamburg       all  2019-01-05T00:50:00+00:00      min  precip_type_5min:idx == 0 & visibility:m       1743.8\n",
    "    \n",
    "            # result = pd.cut(df[\"field_value\"],bins=bins, labels=False )\n",
    "\n",
    "            # for _, row in df.iterrows():\n",
    "            #     pos = math.floor((row[\"field_value\"] - field_min)/stepSize)\n",
    "            #     bins[pos] = bins[pos] + 5\n",
    "        result_df.to_parquet(category[\"name\"] + \"_\" + config[\"location\"] + \".parquet\") #Rain_Hamburg.parquet\n",
    "        result_df.to_csv(category[\"name\"] + \"_\" + config[\"location\"] + \"neu.csv\")\n",
    "\n",
    "        for i,threshold in enumerate(category[\"thresholds\"]):\n",
    "                threshold_results[i][\"above\"] +=timedelta(minutes=5*len(result_df[result_df[\"field_value\"] > threshold]))\n",
    "                threshold_results[i][\"below\"] += timedelta(minutes=5 * len(result_df[result_df[\"field_value\"] < threshold]))\n",
    "                threshold_results[i][\"equal\"] += timedelta(minutes=5 * len(result_df[result_df[\"field_value\"] == threshold]))\n",
    "\n",
    "        # if a== 0:\n",
    "        #     result_df = pd.read_parquet(\"Snow_\" + config[\"location\"] + \".parquet\")\n",
    "        # else:\n",
    "        #     result_df = pd.read_parquet(\"Rain_\" + config[\"location\"] + \".parquet\")\n",
    "\n",
    "        with plt.ioff():\n",
    "            textstr = \"Parameter: {}\\nAggregate: {}\\nLocation: {}\\nArea: {}\\nStart: {}\\nEnd: {}\\n\".format(\n",
    "                readableField, aggregate, config[\"location\"], config[\"area\"], config[\"start_date\"], config[\"end_date\"])\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.set_xlim(left=field_min, right=field_max) # 0,4\n",
    "\n",
    "            if \"max_display_value_y\" in category.keys() and \"min_display_value_y\" in category.keys():\n",
    "                ax.set_ylim(bottom=category[\"min_display_value_y\"], top=category[\"max_display_value_y\"])\n",
    "            #ax.stairs(bins, [field_min+ i * stepSize for i in range(0, math.ceil((field_max - field_min)/stepSize)+1)], fill=True)\n",
    "\n",
    "            # orig -> bottom=True -> y-Werte starten ab 1\n",
    "            # ax.hist(result_df[\"field_value\"], bins, weights=[1/12]*len(result_df.index), bottom=True, align='mid') # left\n",
    "            # edit\n",
    "            ax.hist(result_df[\"field_value\"], bins, weights=[1/12]*len(result_df.index), align='mid') # left\n",
    "            # print(bins) # bei i = 1 -> [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0]\n",
    "            # print(bins) # bei i = 0 -> [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0]\n",
    "            # print(weights=[1/12]*len(result_df.index))\n",
    "            # print(len(result_df.index)) # 314200 länge Datensatz\n",
    "            # print([1/12]*len(result_df.index)) # [0.08333333333333333, 0.08333333333333333, 0.08333333333333333,...]\n",
    "            if \"thresholds\" in category.keys():\n",
    "                for i, threshold in enumerate(category[\"thresholds\"]):\n",
    "                    ax.axvline(threshold, color=\"red\")\n",
    "                    plt.text(threshold, ax.get_ylim()[1] *1.05, str(threshold), ha='center', va='center')\n",
    "                    textstr += \"Threshold: {}\\n  Below: {}\\n  Above: {}\\n  Equal: {}\\n\".format(threshold, threshold_results[i][\"below\"],threshold_results[i][\"above\"], threshold_results[i][\"equal\"] )\n",
    "            plt.text(0.02, 0.3, textstr, fontsize=14, transform=plt.gcf().transFigure)\n",
    "            plt.subplots_adjust(left=0.35, bottom=0.15)\n",
    "            #ax.set_xticks(bins, rotation=90)\n",
    "            # orig:\n",
    "            # bins2 = [bin for i,bin in enumerate(bins) if i % 2 != 0]\n",
    "            # edit:\n",
    "            bins2 = [bin for i,bin in enumerate(bins) if i % 2 != 1] # The % is called the modulo operator. Of course when the remainder is 0, the number is even.\n",
    "            plt.xticks(bins2, bins2, rotation=90)\n",
    "            ax.set_title(\" - \".join([config[\"location\"], config[\"area\"]]), pad=30)\n",
    "            if readableField is not None:\n",
    "                ax.set_xlabel(readableField)\n",
    "            else:\n",
    "                ax.set_xlabel(field)\n",
    "            ax.set_ylabel(\"Hours from {} to {}\".format(config[\"start_date\"][:10], config[\"end_date\"][:10]))\n",
    "            # manager = plt.get_current_fig_manager()\n",
    "            # manager.frame.Maximize(True)\n",
    "            #plt.plot()\n",
    "            #plt.show()\n",
    "            plt.savefig(category[\"name\"] + \"_\" + config[\"location\"] + \"_.png\")\n",
    "            quit()\n",
    "            a += 1\n",
    "\n",
    "# Helferfunktion von aggregateTime()\n",
    "def collect(file):\n",
    "    df = pd.DataFrame(columns=[\"Location\", \"Area\", \"Event\", \"Start\", \"End\", \"Below/above\"])\n",
    "    thresholdLinesAfterEvent = 0\n",
    "    currEntry = {}\n",
    "    for line in file:\n",
    "        result = re.match(eventStartPattern, line)\n",
    "        if result is not None:\n",
    "            currEntry[\"Location\"] = result.group(1)\n",
    "            currEntry[\"Area\"] = result.group(2)\n",
    "            currEntry[\"Event\"] = result.group(3)\n",
    "            currEntry[\"Start\"] = result.group(4)\n",
    "            thresholdLinesAfterEvent = 2\n",
    "        elif thresholdLinesAfterEvent > 0:\n",
    "            result = re.match(thresholdPattern, line)\n",
    "            if result is None:\n",
    "                continue\n",
    "            if thresholdLinesAfterEvent == 2:\n",
    "                try:\n",
    "                    if float(result.group(1)) <= float(result.group(2)):\n",
    "                        currEntry[\"Below/above\"] = \"below\"\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            elif thresholdLinesAfterEvent == 1:\n",
    "                try:\n",
    "                    if float(result.group(1)) >= float(result.group(2)):\n",
    "                        currEntry[\"Below/above\"] = \"above\"\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            thresholdLinesAfterEvent = thresholdLinesAfterEvent - 1\n",
    "        elif currEntry != {}:\n",
    "            result = re.match(eventEndPattern, line)\n",
    "            if result is not None:\n",
    "                currEntry[\"End\"] = result.group(4)\n",
    "                currEntry = {k: [v] for k,v in currEntry.items()}\n",
    "                df = pd.concat([df, pd.DataFrame.from_dict(currEntry)], ignore_index=True)\n",
    "                currEntry = {}\n",
    "    return df\n",
    "\n",
    "def getDate(date_str):\n",
    "    # Correct the offset if there is only one digit for hour\n",
    "    # we need two cases as the offset can be stated with or without colons\n",
    "    if re.search(r\"(\\+|-)\\d:\\d\\d$\", date_str) is not None:\n",
    "        date_str = date_str[:-4] + \"0\" + date_str[-4:]\n",
    "    elif re.search(r\"(\\+|-)\\d\\d\\d$\", date_str) is not None:\n",
    "        date_str = date_str[:-3] + \"0\" + date_str[-3:]\n",
    "    if re.search(r\"(\\+|-)\\d\\d:?\\d\\d$\", date_str) is None:\n",
    "        raise ValueError(\"Unknown way of expressing offset in \" + date_str )\n",
    "    try:\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%dT%H:%M:%S %Z%z')\n",
    "    except ValueError:\n",
    "        date = datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    return date\n",
    "\n",
    "def aggregateTime():\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "    config = yaml.safe_load(open(\"Diagram_Config_T.yml\", \"r\"))\n",
    "    location = config[\"location\"]\n",
    "    area = config[\"area\"]\n",
    "    # print(config[\"events\"])\n",
    "    # {'event_rain': {'name': 'Rain Event', 'mode': 'month', 'high_threshold_field': 'precip_type_5min:idx == 1 & precip_5min:mm', 'high_threshold_agg': 'max', 'high_threshold_value': 1.67, 'downtime_before': '01:00:00', 'downtime_after': '01:00:00', 'downTime': False, 'direction': 'above'}}\n",
    "    \n",
    "    # {'event_visibility_noPrec': {'name': 'Visibility', 'mode': 'month',\n",
    "    # 'low_threshold_field': 'precip_type_5min:idx == 0 & visibility:m',\n",
    "    # 'low_threshold_agg': 'min', 'low_threshold_value': 244, 'downtime_before': '01:00:00',\n",
    "    # 'downtime_after': '01:00:00', 'downTime': True, 'direction': 'below'}}\n",
    "\n",
    "    for event in config[\"events\"]:\n",
    "        monthDistribution = {x+1 : 0 for x in range(0,12)}\n",
    "        hourDistribution = {x: 0 for x in range(0,24)}\n",
    "        # print(monthDistribution) # {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0}\n",
    "        # {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0}\n",
    "        # print(config[\"events\"][event][\"downTime\"]) # True or False\n",
    "        if config[\"events\"][event][\"downTime\"]:\n",
    "            downtimeIncluded = \"with_downtime\"\n",
    "        else:\n",
    "            downtimeIncluded = \"no_downtime\"\n",
    "        duration = timedelta()\n",
    "        # print(duration) # 0:00:00\n",
    "        # print(os.listdir(config[\"file_location\"]))\n",
    "        # ['HH_vis_rain_all_P0.txt', 'HH_vis_rain_all_P0_events.parquet', 'HH_vis_rain_all_P0_Hamburg_all.parquet',\n",
    "        # 'HH_vis_rain_all_P1.txt', 'HH_vis_rain_all_P10.txt', 'HH_vis_rain_all_P10_events.parquet',\n",
    "        # 'HH_vis_rain_all_P10_Hamburg_all.parquet', 'HH_vis_rain_all_P11.txt', 'HH_vis_rain_all_P11_events.parquet',\n",
    "        # 'HH_vis_rain_all_P11_Hamburg_all.parquet', 'HH_vis_rain_all_P12.txt', 'HH_vis_rain_all_P12_events.parquet',\n",
    "        # 'HH_vis_rain_all_P12_Hamburg_all.parquet', 'HH_vis_rain_all_P13.txt', 'HH_vis_rain_all_P13_events.parquet',\n",
    "        # 'HH_vis_rain_all_P13_Hamburg_all.parquet', 'HH_vis_rain_all_P14.txt', 'HH_vis_rain_all_P14_events.parquet',\n",
    "        #  'HH_vis_rain_all_P14_Hamburg_all.parquet', 'HH_vis_rain_all_P15.txt', 'HH_vis_rain_all_P15_events.parquet',\n",
    "        # 'HH_vis_rain_all_P15_Hamburg_all.parquet', 'HH_vis_rain_all_P16.txt', 'HH_vis_rain_all_P16_events.parquet',\n",
    "        #  'HH_vis_rain_all_P16_Hamburg_all.parquet', 'HH_vis_rain_all_P17.txt', 'HH_vis_rain_all_P17_events.parquet',\n",
    "        #  'HH_vis_rain_all_P17_Hamburg_all.parquet', 'HH_vis_rain_all_P18.txt', 'HH_vis_rain_all_P18_events.parquet',\n",
    "        #  'HH_vis_rain_all_P18_Hamburg_all.parquet', 'HH_vis_rain_all_P19.txt', 'HH_vis_rain_all_P19_events.parquet',\n",
    "        #  'HH_vis_rain_all_P19_Hamburg_all.parquet', 'HH_vis_rain_all_P1_events.parquet',\n",
    "        # 'HH_vis_rain_all_P1_Hamburg_all.parquet', 'HH_vis_rain_all_P2.txt', 'HH_vis_rain_all_P2_events.parquet',\n",
    "        # 'HH_vis_rain_all_P2_Hamburg_all.parquet', 'HH_vis_rain_all_P3.txt', 'HH_vis_rain_all_P3_events.parquet',\n",
    "        # 'HH_vis_rain_all_P3_Hamburg_all.parquet', 'HH_vis_rain_all_P4.txt', 'HH_vis_rain_all_P4_events.parquet',\n",
    "        # 'HH_vis_rain_all_P4_Hamburg_all.parquet', 'HH_vis_rain_all_P5.txt', 'HH_vis_rain_all_P5_events.parquet',\n",
    "        # 'HH_vis_rain_all_P5_Hamburg_all.parquet', 'HH_vis_rain_all_P6.txt', 'HH_vis_rain_all_P6_events.parquet',\n",
    "        # 'HH_vis_rain_all_P6_Hamburg_all.parquet', 'HH_vis_rain_all_P7.txt', 'HH_vis_rain_all_P7_events.parquet',\n",
    "        # 'HH_vis_rain_all_P7_Hamburg_all.parquet', 'HH_vis_rain_all_P8.txt', 'HH_vis_rain_all_P8_events.parquet',\n",
    "        # 'HH_vis_rain_all_P8_Hamburg_all.parquet', 'HH_vis_rain_all_P9.txt', 'HH_vis_rain_all_P9_events.parquet',\n",
    "        #  'HH_vis_rain_all_P9_Hamburg_all.parquet']\n",
    "        for dataFileName in os.listdir(config[\"file_location\"]):\n",
    "            if not dataFileName.endswith(\"events.parquet\"):\n",
    "                continue\n",
    "\n",
    "            print(dataFileName.rsplit(\"_\", 1)[0] + \".txt\") #HH_vis_rain_all_P0.txt\n",
    "            with open(os.path.join(config[\"file_location\"], dataFileName.rsplit(\"_\", 1)[0] + \".txt\")) as f:\n",
    "                logEvents = collect(f)\n",
    "                # print(\"logEvents1\")\n",
    "                # print(logEvents)\n",
    "                    # Location Area                     Event                Start                  End Below/above\n",
    "                    # 0    Hamburg  all         event_temperature  2018-12-31T00:00:00  2018-12-31T00:15:00       below\n",
    "                    # 1    Hamburg  all         event_temperature  2019-01-01T16:05:00  2019-01-15T09:45:00       below\n",
    "                    # 2    Hamburg  all         event_temperature  2019-01-19T03:40:00  2019-01-04T14:50:00       below\n",
    "                    # 3    Hamburg  all         event_temperature  2019-01-22T15:00:00  2019-01-22T15:00:00       below\n",
    "                    # 4    Hamburg  all         event_temperature  2019-01-05T16:05:00  2019-01-06T00:00:00       below\n",
    "\n",
    "                logEvents = logEvents[(logEvents[\"Area\"] == area) & (logEvents[\"Location\"] == location)\n",
    "                                    & (logEvents[\"Event\"] == event)]\n",
    "                \n",
    "                print(\"logEvents2\")\n",
    "                print(event) # event_rain\n",
    "                print(logEvents[\"Event\"])\n",
    "                print(logEvents)\n",
    "                #   Location Area         Event                Start                  End Below/above\n",
    "                #     19  Hamburg  all  event_rain_a  2019-01-13T02:45:00  2019-01-13T02:50:00       above\n",
    "                #     20  Hamburg  all  event_rain_a  2019-01-13T06:05:00  2019-01-13T06:10:00       above\n",
    "                #     21  Hamburg  all  event_rain_a  2019-01-13T06:35:00  2019-01-13T06:50:00       above\n",
    "                #     22  Hamburg  all  event_rain_a  2019-01-13T14:20:00  2019-01-13T15:20:00       above\n",
    "                #     23  Hamburg  all  event_rain_a  2019-01-13T16:40:00  2019-01-13T16:45:00       above\n",
    "\n",
    "            df = pd.read_parquet(os.path.join(config[\"file_location\"],dataFileName))\n",
    "            # print(df)\n",
    "            #               type location_name area_name                event_name                 start_datetime                   end_datetime\n",
    "            # 0    with_downtime       Hamburg       all         event_temperature  2018-12-31T00:00:00 UTC+01:00  2018-12-31T02:15:00 UTC+01:00\n",
    "            # 1    with_downtime       Hamburg       all         event_temperature  2019-01-01T16:05:00 UTC+01:00  2019-01-15T11:45:00 UTC+01:00\n",
    "            # 2    with_downtime       Hamburg       all         event_temperature  2019-01-05T16:05:00 UTC+01:00  2019-01-06T02:00:00 UTC+01:00\n",
    "            # 3    with_downtime       Hamburg       all         event_temperature  2019-01-06T23:15:00 UTC+01:00  2019-01-07T03:15:00 UTC+01:00\n",
    "            # 4    with_downtime       Hamburg       all         event_temperature  2019-01-08T15:05:00 UTC+01:00  2019-01-10T08:00:00 UTC+01:00\n",
    "            \n",
    "            # print(len(df.index)) # 223, 251,367\n",
    "\n",
    "\n",
    "            df = df[(df[\"location_name\"] == location) & ( df[\"area_name\"] == area)\n",
    "                & (df[\"type\"] == downtimeIncluded) & (df[\"event_name\"] == event)]\n",
    "            \n",
    "            if len(df.index) == 0:\n",
    "                print('NO ELIGIBLE ROWS FOR FILE')\n",
    "                continue\n",
    "                \n",
    "                #         type location_name area_name    event_name                 start_datetime                   end_datetime\n",
    "                # 87   no_downtime       Hamburg       all  event_rain_a  2019-01-07T22:55:00 UTC+01:00  2019-01-07T23:00:00 UTC+01:00\n",
    "                # 88   no_downtime       Hamburg       all  event_rain_a  2019-01-08T03:30:00 UTC+01:00  2019-01-08T04:10:00 UTC+01:00\n",
    "                # 89   no_downtime       Hamburg       all  event_rain_a  2019-01-08T04:15:00 UTC+01:00  2019-01-08T04:20:00 UTC+01:00\n",
    "                # 90   no_downtime       Hamburg       all  event_rain_a  2019-01-08T04:45:00 UTC+01:00  2019-01-08T05:20:00 UTC+01:00\n",
    "                # 91   no_downtime       Hamburg       all  event_rain_a  2019-01-08T05:45:00 UTC+01:00  2019-01-08T06:15:00 UTC+01:00\n",
    "\n",
    "\n",
    "            # add 5 minutes to current time starting from event start until end\n",
    "            # localise and get month/hour\n",
    "            # print(\"df ->\")\n",
    "            # print(df) # Empty DataFrame\n",
    "            # print(\"before for\")\n",
    "            for _, row in df.iterrows():\n",
    "                # print(\"after for\")\n",
    "                # start = datetime.strptime(row[\"start_datetime\"], \"%Y-%m-%dT%H:%M:%S %Z%z\")\n",
    "                # end = datetime.strptime(row[\"end_datetime\"], \"%Y-%m-%dT%H:%M:%S %Z%z\")\n",
    "                start = getDate(row[\"start_datetime\"])\n",
    "                end = getDate(row[\"end_datetime\"])\n",
    "\n",
    "                # print(start)\n",
    "                # HH_vis_rain_all_P0.txt\n",
    "                # 2019-01-07 22:55:00+01:00\n",
    "                # 2019-01-08 03:30:00+01:00\n",
    "                # 2019-01-08 04:15:00+01:00\n",
    "                # 2019-01-08 04:45:00+01:00\n",
    "                # 2019-01-08 05:45:00+01:00\n",
    "                # 2019-01-08 06:40:00+01:00\n",
    "                # print(end)\n",
    "                # 2019-01-07 23:00:00+01:00\n",
    "                # 2019-01-08 04:10:00+01:00\n",
    "                # 2019-01-08 04:20:00+01:00\n",
    "\n",
    "                # XXX POTENTIAL BIG BUG: this might count each event once per (5 min?) time interval within it!\n",
    "                direction = logEvents[(logEvents[\"Start\"] >= (start-start.utcoffset()).strftime(\"%Y-%m-%dT%H:%M:%S\")) &\n",
    "                          (logEvents[\"End\"] <= (end - end.utcoffset()).strftime(\"%Y-%m-%dT%H:%M:%S\"))][\"Below/above\"].unique()\n",
    "                # print(logEvents[\"Start\"])\n",
    "                # 19    2019-01-13T02:45:00\n",
    "                # 20    2019-01-13T06:05:00\n",
    "                # 21    2019-01-13T06:35:00\n",
    "\n",
    "                # print((start-start.utcoffset()).strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
    "                # 2019-01-07T21:55:00\n",
    "                # 2019-01-08T02:30:00\n",
    "                # 2019-01-08T03:15:00\n",
    "                # 2019-01-08T03:45:00\n",
    "\n",
    "                # print(direction) \n",
    "                # ['above']\n",
    "                # ['above']\n",
    "\n",
    "                if len(direction) != 1:\n",
    "                    # print(row)\n",
    "                    # print(\"multiple directions\")\n",
    "                    continue\n",
    "                else:\n",
    "                    direction = direction[0]\n",
    "                if direction != config[\"events\"][event][\"direction\"]:\n",
    "                    # print(\"wrong direction\")\n",
    "                    continue\n",
    "\n",
    "                duration += end -start\n",
    "                # print(duration)\n",
    "                # 0:05:00\n",
    "                # 0:45:00\n",
    "                # 0:50:00\n",
    "                # 1:25:00\n",
    "                # 1:55:00\n",
    "                # 2:50:00\n",
    "\n",
    "                minutes = 0\n",
    "                curr = start\n",
    "                while curr < end:\n",
    "                    monthDistribution[curr.month] += 5\n",
    "                    # print(monthDistribution[curr.month])\n",
    "                    # 5\n",
    "                    # 10\n",
    "                    # 15\n",
    "                    # 20\n",
    "                    # 25\n",
    "                    # 30\n",
    "                    # 35\n",
    "                    # 40\n",
    "                    hourDistribution[curr.hour] +=5\n",
    "                    # print(hourDistribution[curr.hour])\n",
    "                    curr = curr + timedelta(minutes=5)\n",
    "                    minutes += 5\n",
    "\n",
    "\n",
    "        # print diagram\n",
    "        print(\"diagram\")\n",
    "        # fig=plt.figure(figsize=(12,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "        fig = plt.figure()\n",
    "        fig, ax = plt.subplots()\n",
    "        fig = matplotlib.pyplot.gcf()\n",
    "        fig.set_size_inches(18.5, 10.5)\n",
    "        # fig.savefig('test2png.png', dpi=100)\n",
    "        # fig, ax = plt.subplots(figsize=(10 10))\n",
    "        \n",
    "        \n",
    "        print(\"diagram2\")\n",
    "\n",
    "        for key, value in monthDistribution.items():\n",
    "            monthDistribution[key] = value/60\n",
    "\n",
    "        for key, value in hourDistribution.items():\n",
    "            hourDistribution[key] = value/60\n",
    "\n",
    "\n",
    "        mode = config[\"events\"][event][\"mode\"]\n",
    "\n",
    "        textstr = \"Location: {}\\nArea: {}\\nDistribution of\\n {}\\nAddtional downtime\\n incl.: {}\\n\".format(location, area, event, downtimeIncluded)\n",
    "        print(\"textstr = \")\n",
    "        print(textstr)\n",
    "        \n",
    "        direction = config[\"events\"][event][\"direction\"]\n",
    "        print(\"direction before if direction = \")\n",
    "        print(direction)\n",
    "        if direction == \"above\":\n",
    "            if \"high_threshold_field\" in config[\"events\"][event].keys():\n",
    "                field = config[\"events\"][event][\"high_threshold_field\"]\n",
    "                if len(field) <= 10:\n",
    "                    textstr += \"High threshold field: \\n \" + config[\"events\"][event][\"high_threshold_field\"] + \"\\n\"\n",
    "                else:\n",
    "                    textstr += \"High threshold field: \\n \"\n",
    "                    i = 0\n",
    "                    for element in field.split('&'):\n",
    "                        if i == 0:\n",
    "                            textstr +=  element + \"\\n\"\n",
    "                            i = i+1\n",
    "                        else:\n",
    "                            textstr += \" &\" + element + \"\\n\"\n",
    "            if \"high_threshold_agg\" in config[\"events\"][event].keys():\n",
    "                textstr += \"High threshold aggregate:\\n \" + config[\"events\"][event][\"high_threshold_agg\"] + \"\\n\"\n",
    "            if \"high_threshold_value\" in config[\"events\"][event].keys():\n",
    "                textstr += \"High threshold value: \" + str(config[\"events\"][event][\"high_threshold_value\"]) + \"\\n\"\n",
    "        elif direction == \"below\":\n",
    "            if \"low_threshold_field\" in config[\"events\"][event].keys():\n",
    "                field = config[\"events\"][event][\"low_threshold_field\"]\n",
    "                if len(field) <= 10:\n",
    "                    textstr += \"Low threshold field: \\n \" + config[\"events\"][event][\"low_threshold_field\"] + \"\\n\"\n",
    "                else:\n",
    "                    textstr += \"Low threshold field: \\n \"\n",
    "                    i = 0\n",
    "                    for element in field.split('&'):\n",
    "                        if i == 0:\n",
    "                            textstr +=  element + \"\\n\"\n",
    "                            i = i+1\n",
    "                        else:\n",
    "                            textstr += \" &\" + element + \"\\n\"\n",
    "            if \"low_threshold_agg\" in config[\"events\"][event].keys():\n",
    "                textstr += \"Low threshold aggregate: \" + config[\"events\"][event][\"low_threshold_agg\"] + \"\\n\"\n",
    "            if \"low_threshold_value\" in config[\"events\"][event].keys():\n",
    "                textstr += \"Low threshold value: \" + str(config[\"events\"][event][\"low_threshold_value\"]) + \"\\n\"\n",
    "        else:\n",
    "            print(\"unknown direction\")\n",
    "            quit()\n",
    "\n",
    "        if \"downtime_before\" in config[\"events\"][event].keys():\n",
    "            textstr += \"Downtime before: \" + config[\"events\"][event][\"downtime_before\"] + \"\\n\"\n",
    "        if \"downtime_after\" in config[\"events\"][event].keys():\n",
    "            textstr += \"Downtime after: \" + config[\"events\"][event][\"downtime_after\"] + \"\\n\"\n",
    "\n",
    "\n",
    "        if mode == \"hour\":\n",
    "            ax.bar(range(len(hourDistribution.keys())), list(hourDistribution.values()), align=\"center\" )\n",
    "            ax.set_xticks(range(len(hourDistribution.keys())),hourDistribution.keys() )\n",
    "            ax.set_xlabel(\"Hours in day\")\n",
    "        elif mode == \"month\":\n",
    "            ax.bar(range(len(monthDistribution.keys())), list(monthDistribution.values()), align=\"center\" )\n",
    "            ax.set_xticks(range(len(monthDistribution.keys())),monthDistribution.keys() )\n",
    "            ax.set_xlabel(\"Months in year\")\n",
    "        ax.set_ylabel(\"Downtime in hours\\nfrom {} to {}\".format(config[\"start_date\"][:10], config[\"end_date\"][:10]))\n",
    "        ax.set_title(\" - \".join([config[\"events\"][event][\"name\"],config[\"location\"], config[\"area\"]]))\n",
    "        plt.text(0.02, 0.3, textstr, fontsize=14, transform=plt.gcf().transFigure)\n",
    "        plt.subplots_adjust(left=0.35, bottom= 0.15)\n",
    "        plt.savefig(config[\"location\"] + '_' + config[\"events\"][event][\"name\"] + '_aggregateTime.png', dpi=100)\n",
    "        print(config[\"location\"])\n",
    "        print(config[\"events\"][event])\n",
    "\n",
    "        manager = plt.get_current_fig_manager()\n",
    "        #manager.window.state('zoomed')\n",
    "        plt.pause(1)\n",
    "\n",
    "        # plt.plot()\n",
    "        # plt.show()\n",
    "\n",
    "def countEventTimes():\n",
    "    # directory = \"WeatherResults\\Hamburg\" # geht nicht da backslash\n",
    "    directory = \"output_pipeline\" # geht\n",
    "\n",
    "    output_df = pd.DataFrame(columns=[\"key\", \"duration\"])\n",
    "\n",
    "    for dataFileName in os.listdir(directory):\n",
    "        if not dataFileName.endswith(\"events.parquet\"):\n",
    "            continue\n",
    "        print(dataFileName)\n",
    "        df = pd.read_parquet(os.path.join(directory, dataFileName))\n",
    "        df.to_csv(dataFileName.split('.')[0] + \".csv\")\n",
    "        durations = []\n",
    "        keys = []\n",
    "        for idx, row in df.iterrows():\n",
    "            if row[\"start_datetime\"][-5] in [\"+\", \"-\"]:\n",
    "                start_string = row[\"start_datetime\"][0:len(row[\"start_datetime\"]) - 5] + row[\"start_datetime\"][-5] \\\n",
    "                + \"0\" + row[\"start_datetime\"][-4:len(row[\"start_datetime\"])]\n",
    "            else:\n",
    "                start_string = row[\"start_datetime\"]\n",
    "            if row[\"end_datetime\"][-5] in [\"+\", \"-\"]:\n",
    "                end_string = row[\"end_datetime\"][0:len(row[\"end_datetime\"]) - 5] + row[\"end_datetime\"][-5] \\\n",
    "                + \"0\" + row[\"end_datetime\"][-4:len(row[\"end_datetime\"])]\n",
    "            else:\n",
    "                end_string = row[\"end_datetime\"]\n",
    "\n",
    "            # start = datetime.strptime(start_string, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "            # end = datetime.strptime(end_string, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "            # start = datetime.strptime(row[\"start_datetime\"], \"%Y-%m-%dT%H:%M:%S %Z%z\")\n",
    "            # end = datetime.strptime(row[\"end_datetime\"], \"%Y-%m-%dT%H:%M:%S %Z%z\")\n",
    "            start = getDate(row[\"start_datetime\"])\n",
    "            end = getDate(row[\"end_datetime\"])\n",
    "\n",
    "            durations.append(end-start)\n",
    "            keys.append(\"_\".join([row[\"event_name\"],row[\"type\"], row[\"location_name\"],row[\"area_name\"]]))\n",
    "        df[\"duration\"] = durations\n",
    "        df[\"key\"] = keys\n",
    "        df = df.drop(['end_datetime', 'start_datetime', 'area_name', 'location_name'], axis=1)\n",
    "        #df = df.groupby([\"key\"])[\"duration\"].sum()\n",
    "        output_df = pd.concat([output_df, df], ignore_index=True)\n",
    "    result = {k: timedelta() for k in output_df[\"key\"].unique()}\n",
    "    #print(output_df.head())\n",
    "    for idx, row in output_df.iterrows():\n",
    "        result[row[\"key\"]] += row[\"duration\"]\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(result)\n",
    "    output_df = output_df.groupby([\"key\"])[[\"duration\"]].sum()\n",
    "    #print(output_df)\n",
    "    print(directory) # WeatherResults\\Hamburg\n",
    "    # print(\"Events_duration\" + directory.split(\"/\")[-1] + \".csv\") # Events_durationWeatherResults\\Hamburg.csv\n",
    "    output_df.to_csv(\"Events_duration\" + directory.split(\"/\")[-1] + \".csv\")    # orig geht nur ohne backslash im directory\n",
    "    # output_df.to_csv(\"Events_duration\" + \"test\" + \".csv\")\n",
    "\n",
    "\n",
    "# interner check\n",
    "def countValue():\n",
    "    # fileLocation = \"WeatherResults\\Hamburg\"\n",
    "    fileLocation = \"results\"\n",
    "    precipSum = 0\n",
    "\n",
    "    for dataFileName in os.listdir(fileLocation):\n",
    "        if not dataFileName.endswith(\"all.parquet\"):\n",
    "            continue\n",
    "        # print(dataFileName) # HH_vis_rain_Hamburg_all.parquet\n",
    "        df = pd.read_parquet(os.path.join(fileLocation, dataFileName))\n",
    "        print(df.head(10))\n",
    "        # df = df[(df[\"field_name\"] == \"precip_type_5min:idx == 1 & precip_5min:mm\") & (df[\"agg_name\"] == \"average\")]\n",
    "        df = df[(df[\"field_name\"] == \"precip_type_5min:idx == 0 & visibility:m\") & (df[\"agg_name\"] == \"min\")] # ????? falsche results!\n",
    "        print(\"before fill NaN\")\n",
    "        print(df.head(10))\n",
    "\n",
    "        # Achtung = bei Sichtweite muss eine hohe Dummy Zahl eingegeben werden!\n",
    "        df.fillna(0, inplace=True) # falsch -> Nan != 0,0\n",
    "        print(\"after fill NaN\")\n",
    "        print(df.head(10))\n",
    "        precipSum += df[\"field_value\"].sum() # 474.9435448843631 (\"precip_type_5min:idx == 1 & precip_5min:mm\")\n",
    "        # precipSum += df[\"field_value\"].mean() # 474.9435448843631 (\"precip_type_5min:idx == 1 & precip_5min:mm\") #???? zu hoher mean\n",
    "        #print(df.head())\n",
    "\n",
    "    print(precipSum)\n",
    "\n",
    "# check ein einem punkt\n",
    "def countPrecipationRaw():\n",
    "    # fileLocation = \"WeatherResults\\Hamburg\"\n",
    "    fileLocation = \"results\"\n",
    "    precipSum = 0\n",
    "\n",
    "    for dataFileName in os.listdir(fileLocation):\n",
    "        print(\"dataFileName = \" + dataFileName)\n",
    "        # dataFileName = HH_vis_rain.txt\n",
    "        # dataFileName = HH_vis_rain.yml\n",
    "        # dataFileName = HH_vis_rain_events.parquet\n",
    "        if not dataFileName.endswith(\".parquet\"):\n",
    "            # print(\"if dataFileName = \" + dataFileName) # HamburgAnalysis_P0(copy)_P0.txt\n",
    "            continue\n",
    "        # if \"2021\" in dataFileName or \"2022\" in dataFileName: # welche txt wird benötigt?\n",
    "        if \"2020\" in dataFileName or \"2022\" in dataFileName: # welche txt wird benötigt?\n",
    "            # print(\"dataFileName\")\n",
    "            # print(dataFileName)\n",
    "            continue\n",
    "        df = pd.read_parquet(os.path.join(fileLocation, dataFileName))\n",
    "        #print(\"df\")\n",
    "        print(df)\n",
    "        #print(df[\"precip_type_5min:idx\"])\n",
    "        df = df[df[\"precip_type_5min:idx\"] == 1] # error\n",
    "        df = df[(df[\"lat\"] == 53.624071) & (df[\"lon\"] == 9.992043)] #airport nähe\n",
    "        print(df.head())\n",
    "        precipSum += df[\"precip_5min:mm\"].sum()\n",
    "\n",
    "    print(precipSum)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    aggregateDiagram_Statistic_all()\n",
    "    aggregateDiagram_rain_snow() # categories\n",
    "    aggregateDiagram_rain_noNaN()\n",
    "    aggregateDiagram_visibility() # \n",
    "    aggregateDiagram_origJohannes() # orig\n",
    "    # aggregateDiagram_all() # allg Übersicht, ohne tresholds->categories\n",
    "    aggregateTime() # *.txt, unterer Teil listet events mit / ohne downtime auf -> rain geht nicht!\n",
    "    countEventTimes() # Pfad directory = anpassen!\n",
    "    countPrecipationRaw() # geht nicht, Input unbekannt!\n",
    "    countValue() # geht -> #realistische Werte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bdbdb4-dd71-462e-af2c-f5e82ec73109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff906eb5-851e-446e-8319-3bac7128214f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ee5ba-ed9b-40a3-b962-9f72661419d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ce760d-10f2-45a8-b907-be0089da1ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
